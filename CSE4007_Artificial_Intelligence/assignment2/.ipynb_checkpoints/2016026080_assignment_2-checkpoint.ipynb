{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from konlpy.tag import Twitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/triplet36/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "/home/ubuntu/anaconda3/envs/triplet36/lib/python3.6/site-packages/jpype/_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "twitter=Twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Big_Data_Center - all tokens from training_samples by twitter tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_BigDataCenter(filename, smoothing_K):\n",
    "    \n",
    "    samples = pd.read_csv(filename, \"\\t\")\n",
    "    documents = samples[\"document\"]\n",
    "    labels = samples[\"label\"]\n",
    "\n",
    "    big_data_center = []\n",
    "    dc_idxs={}\n",
    "\n",
    "    for idx in range(len(documents)):\n",
    "        if idx%500 == 0:\n",
    "            print(idx)\n",
    "        document=documents[idx]\n",
    "        label=labels[idx]\n",
    "\n",
    "        # filter nan\n",
    "        if(document!=document):\n",
    "            continue\n",
    "\n",
    "        tokens = ['/'.join(t) for t in twitter.pos(document, norm=True, stem=True)]\n",
    "        repeat={}\n",
    "        for token in tokens:\n",
    "            if token in repeat:\n",
    "                continue\n",
    "            repeat[token]=1\n",
    "\n",
    "            if token in dc_idxs:\n",
    "                big_data_center[dc_idxs[token]][\"counts\"]+=1\n",
    "                big_data_center[dc_idxs[token]][\"counts_H\" if label==1 else \"counts_NH\"]+=1\n",
    "            else:\n",
    "                dc_idxs[token]=len(big_data_center)\n",
    "                big_data_center.append({\"parameter\":token, \"counts\":1, \"counts_H\":0, \"counts_NH\":0, \"p_from_H\":0, \"p_from_NH\":0, \"priority\":0})\n",
    "                big_data_center[dc_idxs[token]][\"counts_H\" if label==1 else \"counts_NH\"]+=1\n",
    "\n",
    "    H_NUM = len(np.where(labels==1)[0])\n",
    "    NH_NUM = len(np.where(labels==0)[0])\n",
    "\n",
    "    for data in big_data_center:\n",
    "        data[\"p_from_H\"]= (data[\"counts_H\"]+smoothing_K)/(H_NUM+2*smoothing_K)\n",
    "        data[\"p_from_NH\"]= (data[\"counts_NH\"]+smoothing_K)/(NH_NUM+2*smoothing_K)\n",
    "        data[\"priority\"]=max([ data[\"counts_H\"],data[\"counts_NH\"] ])/(data[\"counts\"]) \n",
    "\n",
    "    sorted_big_data_center = sorted(big_data_center, key=lambda data:(data[\"counts\"], data[\"priority\"]))\n",
    "    sorted_big_data_center.reverse()\n",
    "    \n",
    "    pd.DataFrame(sorted_big_data_center).to_csv(\"naive_bassian_models/BigDataCenter.csv\")\n",
    "    \n",
    "    for data in sorted_big_data_center[:10]:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Small_Data_Center only has useful parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DataCenter(filename, num_of_parameter, min_counts, min_priority):\n",
    "    big_data_center_pd = pd.read_csv(\"naive_bassian_models/BigDataCenter.csv\")\n",
    "    priorities=big_data_center_pd[\"priority\"]\n",
    "    columns=big_data_center_pd.columns\n",
    "    idxs=np.where(priorities>=min_priority)[0].astype(int)\n",
    "\n",
    "    data_center=[]\n",
    "    for idx in idxs:\n",
    "        if (len(data_center)==num_of_parameter) or (big_data_center_pd[\"counts\"][idx]<min_counts):\n",
    "            break\n",
    "\n",
    "        node ={}\n",
    "        for column in columns:\n",
    "            node[column]=big_data_center_pd[column][idx]\n",
    "        data_center.append(node)\n",
    "\n",
    "    pd.DataFrame(data_center).to_csv(filename)\n",
    "    print(\"Make Datacenter with len: \"+str(len(data_center)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Model with Validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model_name, sample_name):\n",
    "    data_center_pd = pd.read_csv(model_name)\n",
    "    parameters=data_center_pd[\"parameter\"]\n",
    "    p_H=data_center_pd[\"p_from_H\"]\n",
    "    p_NH=data_center_pd[\"p_from_NH\"]\n",
    "    \n",
    "    samples=pd.read_csv(sample_name, \"\\t\")\n",
    "    documents=samples[\"document\"]\n",
    "    labels=samples[\"label\"]\n",
    "    \n",
    "    score=0\n",
    "    for idx in range(len(documents)):\n",
    "        if idx%50 == 0:\n",
    "            print(idx, score)\n",
    "        document=documents[idx]\n",
    "        label=labels[idx]\n",
    "        if get_label(parameters, p_H, p_NH, document)==label:\n",
    "            score+=1\n",
    "    \n",
    "    return score*100/len(documents)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(parameters, P_H, P_NH, document):\n",
    "    if document != document:\n",
    "        return 1\n",
    "    \n",
    "    MULFACTOR=10000\n",
    "    MINFACTOR=0.000001\n",
    "    p_H=math.log(MULFACTOR*89966/180000)\n",
    "    p_NH=math.log(MULFACTOR*90034/180000)\n",
    "    tokens = ['/'.join(t) for t in twitter.pos(document, norm=True, stem=True)]\n",
    "    for idx in range(len(parameters)):\n",
    "        parameter=parameters[idx]\n",
    "        if parameter in tokens:\n",
    "            p_H += math.log(MULFACTOR * max([P_H[idx], MINFACTOR]))\n",
    "            p_NH += math.log(MULFACTOR * max([P_NH[idx], MINFACTOR]))\n",
    "        else:\n",
    "            p_H += math.log(MULFACTOR * max([1-P_H[idx], MINFACTOR]))\n",
    "            p_NH += math.log(MULFACTOR * max([1-P_NH[idx], MINFACTOR]))\n",
    "            \n",
    "    return 1 if p_H>=p_NH else 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make DataCenter with parameters and Optimize parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters():\n",
    "    scores=[]\n",
    "    for PARAMS in [1000,1100,1200,1300,1400]:\n",
    "        for MIN_COUNTS in [300, 250, 200, 150, 100, 80]:\n",
    "            for MIN_PRIORITY in [0.65, 0.64, 0.63, 0.62, 0.61, 0.60, 0.59, 0.58, 0.57, 0.56, 0.55]:\n",
    "\n",
    "                #PARAMS=1000\n",
    "                #MIN_COUNTS=100\n",
    "                #MIN_PRIORITY=0.62\n",
    "                model_name=\"naive_bassian_models/data_center_cases/data_center_\"+str(PARAMS)+\"_\"+str(MIN_COUNTS)+\"_\"+str(MIN_PRIORITY)+\".csv\"\n",
    "                make_DataCenter(model_name, PARAMS, MIN_COUNTS, MIN_PRIORITY)\n",
    "                validation_sample = \"ratings_valid.txt\"\n",
    "                score = score_model(model_name, validation_sample)\n",
    "                print(score)\n",
    "                scores.append({\"combination\":str(PARAMS)+\"_\"+str(MIN_COUNTS)+\"_\"+str(MIN_PRIORITY), \"score\": score})\n",
    "\n",
    "    scores_pd = pd.DataFrame(scores)\n",
    "    scores_pd.to_csv(\"naive_bassian_models/Result_optimizing_parameters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict samples with trainined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_name, sample_name):\n",
    "    data_center_pd = pd.read_csv(model_name)\n",
    "    parameters=data_center_pd[\"parameter\"]\n",
    "    p_H=data_center_pd[\"p_from_H\"]\n",
    "    p_NH=data_center_pd[\"p_from_NH\"]\n",
    "    \n",
    "    samples=pd.read_csv(sample_name, \"\\t\")\n",
    "    documents=samples[\"document\"]\n",
    "    labels=[]\n",
    "    \n",
    "    for idx in range(len(documents)):\n",
    "        if idx%50==0:\n",
    "            print(idx)\n",
    "        document=documents[idx]\n",
    "        label=get_label(parameters, p_H, p_NH, document)\n",
    "        labels.append(label)\n",
    "    \n",
    "    labels_pd=pd.Series(labels,dtype=\"int32\")\n",
    "        \n",
    "    samples[\"label\"] = labels_pd\n",
    "    samples.astype({'label':'int'})\n",
    "    samples.to_csv(sample_name.split(\"_\")[0]+\"_result.txt\")\n",
    "    print(samples.dtypes)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate BigDataCenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample=\"ratings_train.txt\"\n",
    "#generate_BigDataCenter(training_sample, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with Validation Samples. PARAMS=1200, MC=100, MP=0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Datacenter with len: 6496\n",
      "0 0\n",
      "50 45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b31ba83939e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#model_name=\"naive_bassian_models/naive_bassian_model.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvalidation_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ratings_valid.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-09222f57b53d>\u001b[0m in \u001b[0;36mscore_model\u001b[0;34m(model_name, sample_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdocument\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_NH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mscore\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-50010768e931>\u001b[0m in \u001b[0;36mget_label\u001b[0;34m(parameters, P_H, P_NH, document)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mparameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mp_H\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMULFACTOR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP_H\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMINFACTOR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/triplet36/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/triplet36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4726\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4728\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4730\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/triplet36/lib/python3.6/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_joined_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PARAMS=50000\n",
    "MIN_COUNTS=20\n",
    "MIN_PRIORITY=0.52\n",
    "\n",
    "model_name=\"naive_bassian_models/data_center_cases/data_center_\"+str(PARAMS)+\"_\"+str(MIN_COUNTS)+\"_\"+str(MIN_PRIORITY)+\".csv\"\n",
    "make_DataCenter(model_name, PARAMS, MIN_COUNTS, MIN_PRIORITY)\n",
    "\n",
    "#model_name=\"naive_bassian_models/naive_bassian_model.csv\"\n",
    "validation_sample = \"ratings_valid.txt\"\n",
    "score = score_model(model_name, validation_sample)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name=\"naive_bassian_models/naive_bassian_model.csv\"\n",
    "test_sample = \"ratings_test.txt\"\n",
    "predict(model_name, test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triplet36",
   "language": "python",
   "name": "triplet36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
