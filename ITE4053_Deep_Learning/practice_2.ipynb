{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "np.random.seed(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self):\n",
    "        for layer in layers:\n",
    "            layer.forward()\n",
    "    \n",
    "    def backward(self):\n",
    "        for layer in self.layers.reversed():\n",
    "            layer.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryClassifier with vector calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class BinaryClassifier:\n",
    "    \n",
    "    def __init__(self, DIM=(1,2)):\n",
    "        self.DIM = DIM\n",
    "        self.w = np.zeros(self.DIM, dtype=np.float64)\n",
    "        self.b = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        MIN_MARGIN = 2 ** -53\n",
    "\n",
    "        self.z = np.dot(self.w, X.T) + self.b\n",
    "        self.a = 1 / (1 + np.exp(-self.z))\n",
    "        self.a = np.maximum(np.minimum(1 - MIN_MARGIN, self.a), MIN_MARGIN)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        da = -y / self.a + (1 - y) / (1 - self.a) #(1,100)\n",
    "        dz = self.a * (1 - self.a) * da           #(1,100)\n",
    "        dw = np.mean(X * dz.T, 0)                 #(2)\n",
    "        db = np.mean(1 * dz)                      #()\n",
    "        return dw, db\n",
    "\n",
    "    def train(self, X, y, learning_rate):\n",
    "        self.forward(X)\n",
    "        dw, db = self.backward(X, y)\n",
    "        self.w -= learning_rate * dw\n",
    "        self.b -= learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.round(self.forward(X))\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        pred_y = self.forward(X)\n",
    "        return -np.mean(y * np.log(pred_y) + (1 - y) * np.log(1 - pred_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryClassifier with elementwise calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class BinaryClassifierElementwise:\n",
    "    \n",
    "    def __init__(self, DIM=(1,2)):\n",
    "        self.DIM = DIM\n",
    "        self.w = np.zeros(self.DIM, dtype=np.float64)\n",
    "        self.b = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        MIN_MARGIN = 2 ** -53\n",
    "\n",
    "        # self.z = np.dot(self.w, X.T) + self.b\n",
    "        tmp_z = []\n",
    "        tmp_a = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            tmp_z.append(self.w[0][0]*X[i][0] + self.w[0][1]*X[i][1] + self.b)\n",
    "            tmp_a.append(1 / (1 + np.exp(-tmp_z[i])))\n",
    "            \n",
    "            tmp_min = min(1 - MIN_MARGIN, tmp_a[i])\n",
    "            tmp_a[i] = tmp_min if tmp_min>MIN_MARGIN else MIN_MARGIN\n",
    "        \n",
    "        self.a = np.array(tmp_a).reshape((1, X.shape[0]))\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \n",
    "        X_nums = X.shape[0]\n",
    "        \n",
    "        da=[[]] #(1,100)\n",
    "        dz=[[]] #(1,100)\n",
    "        dw=[0.0, 0.0] #(2)\n",
    "        db=0.0 #()\n",
    "        for i in range(X_nums):\n",
    "            da[0].append(-y[i] / self.a[0][i] + (1 - y[i]) / (1 - self.a[0][i]))\n",
    "            dz[0].append(self.a[0][i] * (1 - self.a[0][i]) * da[0][i])\n",
    "            dw[0] += X[i][0] * dz[0][i]\n",
    "            dw[1] += X[i][1] * dz[0][i]\n",
    "            db += 1 * dz[0][i]\n",
    "            \n",
    "        dw[0]/=X_nums\n",
    "        dw[1]/=X_nums\n",
    "        db/=X_nums\n",
    "        return np.array(dw), db\n",
    "\n",
    "    def train(self, X, y, learning_rate):\n",
    "        self.forward(X)\n",
    "        dw, db = self.backward(X, y)\n",
    "        self.w[0][0] -= learning_rate * dw[0]\n",
    "        self.w[0][1] -= learning_rate * dw[1]\n",
    "        self.b -= learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.round(self.forward(X))\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        pred_y = self.forward(X)\n",
    "        return -np.mean(y * np.log(pred_y) + (1 - y) * np.log(1 - pred_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(sample_num):\n",
    "    X = np.random.randint(-10, 10, (sample_num, 2))\n",
    "    y = (np.sum(X, 1) >0).astype(int)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_samples, test_samples, learning_rate, epochs, option='NULL'):\n",
    "    \n",
    "    # model = Model([BinaryClassifier((1,2))])\n",
    "    classifier = BinaryClassifier((1,2))\n",
    "    train_X, train_y = create_samples(train_samples)\n",
    "    test_X, test_y = create_samples(test_samples)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        classifier.train(train_X, train_y, learning_rate)\n",
    "        if option == 'print':\n",
    "            print('\\nepoch #' + str(epoch+1))\n",
    "            print('w :' + str(classifier.w.reshape((1,2))))\n",
    "            print('b : ' + str(classifier.b))\n",
    "            \n",
    "    return {'w': classifier.w.reshape((1,2)), \n",
    "            'b': classifier.b,\n",
    "            'train_loss': classifier.loss(train_X, train_y),\n",
    "            'test_loss': classifier.loss(test_X, test_y),\n",
    "            'train_acc': 100 * np.mean(classifier.predict(train_X) == train_y),\n",
    "            'test_acc': 100 * np.mean(classifier.predict(test_X) == test_y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_elementwise(train_samples, test_samples, learning_rate, epochs, option='NULL'):\n",
    "    \n",
    "    # model = Model([BinaryClassifier((1,2))])\n",
    "    classifier = BinaryClassifierElementwise((1,2))\n",
    "    train_X, train_y = create_samples(train_samples)\n",
    "    test_X, test_y = create_samples(test_samples)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        classifier.train(train_X, train_y, learning_rate)\n",
    "        if option == 'print':\n",
    "            print('\\nepoch #' + str(epoch+1))\n",
    "            print('w :' + str(classifier.w.reshape((1,2))))\n",
    "            print('b : ' + str(classifier.b))\n",
    "                \n",
    "    return {'w': classifier.w.reshape((1,2)), \n",
    "            'b': classifier.b,\n",
    "            'train_loss': classifier.loss(train_X, train_y),\n",
    "            'test_loss': classifier.loss(test_X, test_y),\n",
    "            'train_acc': 100 * np.mean(classifier.predict(train_X) == train_y),\n",
    "            'test_acc': 100 * np.mean(classifier.predict(test_X) == test_y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': array([[0.37411396, 0.45177562]]),\n",
       " 'b': -0.018992888908487472,\n",
       " 'train_loss': 0.17186290637380514,\n",
       " 'test_loss': 0.1494035394228638,\n",
       " 'train_acc': 96.0,\n",
       " 'test_acc': 97.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = 100 # m\n",
    "test_samples = 100 # n\n",
    "learning_rate = 1e-2\n",
    "epochs = 100 # K\n",
    "\n",
    "train(train_samples, test_samples, learning_rate, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print (w, a) each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #1\n",
      "w :[[0.0149  0.01935]]\n",
      "b : -0.0008\n",
      "\n",
      "epoch #2\n",
      "w :[[0.02851465 0.03683198]]\n",
      "b : -0.001522395733802674\n",
      "\n",
      "epoch #3\n",
      "w :[[0.0409868  0.05266626]]\n",
      "b : -0.002177053725053515\n",
      "\n",
      "epoch #4\n",
      "w :[[0.05246057 0.06706863]]\n",
      "b : -0.0027739373166111815\n",
      "\n",
      "epoch #5\n",
      "w :[[0.06306875 0.08023544]]\n",
      "b : -0.0033221518822761807\n",
      "\n",
      "epoch #6\n",
      "w :[[0.07292775 0.09233793]]\n",
      "b : -0.0038295645305330295\n",
      "\n",
      "epoch #7\n",
      "w :[[0.08213682 0.10352202]]\n",
      "b : -0.004302764188099197\n",
      "\n",
      "epoch #8\n",
      "w :[[0.09077951 0.11391059]]\n",
      "b : -0.004747181237354083\n",
      "\n",
      "epoch #9\n",
      "w :[[0.09892576 0.12360667]]\n",
      "b : -0.0051672605426926616\n",
      "\n",
      "epoch #10\n",
      "w :[[0.10663414 0.13269667]]\n",
      "b : -0.005566635779194124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w': array([[0.10663414, 0.13269667]]),\n",
       " 'b': -0.005566635779194124,\n",
       " 'train_loss': 0.3994439835611875,\n",
       " 'test_loss': 0.4005176221978117,\n",
       " 'train_acc': 98.0,\n",
       " 'test_acc': 100.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### epochs를 100으로 설정하면 결과값이 너무 길어지기 때문에, 10으로 설정하고 돌린다.\n",
    "train(train_samples, test_samples, learning_rate, 10, option='print')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### element-wise version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5849499702453613\n",
      "{'w': array([[0.41454156, 0.40819955]]), 'b': -0.04905599544328502, 'train_loss': 0.1466250264640928, 'test_loss': 0.15289553856197846, 'train_acc': 98.0, 'test_acc': 100.0}\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "result_elementwise = train_elementwise(train_samples, test_samples, learning_rate, epochs)\n",
    "print(time.time()-start)\n",
    "print(result_elementwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012872934341430664\n",
      "{'w': array([[0.42484946, 0.41395356]]), 'b': -0.08788218729060701, 'train_loss': 0.13828093278404657, 'test_loss': 0.15607500607482863, 'train_acc': 100.0, 'test_acc': 100.0}\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "result_vector = train(train_samples, test_samples, learning_rate, epochs)\n",
    "print(time.time()-start)\n",
    "print(result_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameter(train_samples, test_samples, learning_rate,  epochs, runs):\n",
    "    results=[]\n",
    "    for run in range(runs):\n",
    "        results.append(train(train_samples, test_samples, learning_rate,  epochs))\n",
    "    \n",
    "    results_df = pd.DataFrame(results)[['train_loss', 'test_loss', 'train_acc', 'test_acc']]\n",
    "    normalized_result = results_df.sum()/runs\n",
    "    \n",
    "    return normalized_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_loss     0.165952\n",
       "test_loss      0.173744\n",
       "train_acc     98.280000\n",
       "test_acc      97.640000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs = 100\n",
    "\n",
    "test_parameter(train_samples, test_samples, learning_rate, epochs, runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameter m (=train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train_samples :  10\n",
      "train_loss     0.14185\n",
      "test_loss      0.22862\n",
      "train_acc     98.40000\n",
      "test_acc      92.31000\n",
      "dtype: float64\n",
      "\n",
      "Train_samples :  100\n",
      "train_loss     0.171549\n",
      "test_loss      0.174781\n",
      "train_acc     98.270000\n",
      "test_acc      97.610000\n",
      "dtype: float64\n",
      "\n",
      "Train_samples :  1000\n",
      "train_loss     0.172270\n",
      "test_loss      0.168548\n",
      "train_acc     98.918000\n",
      "test_acc      98.970000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for m in [10,100,1000]:\n",
    "    print(\"\\nTrain_samples : \", m)\n",
    "    print(test_parameter(m, test_samples, learning_rate, epochs, runs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameter K (=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs :  10\n",
      "train_loss     0.397752\n",
      "test_loss      0.405442\n",
      "train_acc     96.630000\n",
      "test_acc      95.760000\n",
      "dtype: float64\n",
      "\n",
      "Epochs :  100\n",
      "train_loss     0.172736\n",
      "test_loss      0.173047\n",
      "train_acc     98.280000\n",
      "test_acc      97.800000\n",
      "dtype: float64\n",
      "\n",
      "Epochs :  1000\n",
      "train_loss     0.074933\n",
      "test_loss      0.078105\n",
      "train_acc     99.560000\n",
      "test_acc      98.960000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for K in [10,100,1000]:\n",
    "    print(\"\\nEpochs : \", K)\n",
    "    print(test_parameter(train_samples, test_samples, learning_rate, K, runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameter lr (=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning_rate :  1e-08\n",
      "train_loss     0.693142\n",
      "test_loss      0.693142\n",
      "train_acc     96.280000\n",
      "test_acc      95.070000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  0.0001\n",
      "train_loss     0.641518\n",
      "test_loss      0.641716\n",
      "train_acc     96.390000\n",
      "test_acc      95.800000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  0.01\n",
      "train_loss     0.167929\n",
      "test_loss      0.175276\n",
      "train_acc     98.290000\n",
      "test_acc      97.390000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  0.1\n",
      "train_loss     0.075399\n",
      "test_loss      0.080807\n",
      "train_acc     99.470000\n",
      "test_acc      98.800000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  1.0\n",
      "train_loss      0.024045\n",
      "test_loss       0.030065\n",
      "train_acc     100.000000\n",
      "test_acc       99.610000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  10.0\n",
      "train_loss     0.004852\n",
      "test_loss      0.031214\n",
      "train_acc     99.930000\n",
      "test_acc      99.500000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-8, 1e-4, 1e-2, 1e-1, 1e0, 1e1]:\n",
    "    print(\"\\nLearning_rate : \", lr)\n",
    "    print(test_parameter(train_samples, test_samples, lr, epochs, runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': array([[2.41652767, 2.3998954 ]]),\n",
       " 'b': -1.0520468981772695,\n",
       " 'train_loss': 0.02426320864479219,\n",
       " 'test_loss': 0.03131281814486276,\n",
       " 'train_acc': 100.0,\n",
       " 'test_acc': 100.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(1000, test_samples, 1.0, epochs, runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
