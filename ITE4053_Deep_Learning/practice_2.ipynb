{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "np.random.seed(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self):\n",
    "        for layer in layers:\n",
    "            layer.forward()\n",
    "    \n",
    "    def backward(self):\n",
    "        for layer in self.layers.reversed():\n",
    "            layer.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryClassifier with vector calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class BinaryClassifier:\n",
    "    \n",
    "    def __init__(self, DIM=(1,2)):\n",
    "        self.DIM = DIM\n",
    "        self.w = np.zeros(self.DIM, dtype=np.float64)\n",
    "        self.b = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        MIN_MARGIN = 2 ** -53\n",
    "\n",
    "        self.z = np.dot(self.w, X.T) + self.b\n",
    "        self.a = 1 / (1 + np.exp(-self.z))\n",
    "        self.a = np.maximum(np.minimum(1 - MIN_MARGIN, self.a), MIN_MARGIN)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        da = -y / self.a + (1 - y) / (1 - self.a) #(1,100)\n",
    "        dz = self.a * (1 - self.a) * da           #(1,100)\n",
    "        dw = np.mean(X * dz.T, 0)                 #(2)\n",
    "        db = np.mean(1 * dz)                      #()\n",
    "        return dw, db\n",
    "\n",
    "    def train(self, X, y, learning_rate):\n",
    "        self.forward(X)\n",
    "        dw, db = self.backward(X, y)\n",
    "        self.w -= learning_rate * dw\n",
    "        self.b -= learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.round(self.forward(X))\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        pred_y = self.forward(X)\n",
    "        return -np.mean(y * np.log(pred_y) + (1 - y) * np.log(1 - pred_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryClassifier with elementwise calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class BinaryClassifierElementwise:\n",
    "    \n",
    "    def __init__(self, DIM=(1,2)):\n",
    "        self.DIM = DIM\n",
    "        self.w = np.zeros(self.DIM, dtype=np.float64)\n",
    "        self.b = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        MIN_MARGIN = 2 ** -53\n",
    "\n",
    "        # self.z = np.dot(self.w, X.T) + self.b\n",
    "        tmp_z = []\n",
    "        tmp_a = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            tmp_z.append(self.w[0][0]*X[i][0] + self.w[0][1]*X[i][1] + self.b)\n",
    "            tmp_a.append(1 / (1 + np.exp(-tmp_z[i])))\n",
    "            \n",
    "            tmp_min = min(1 - MIN_MARGIN, tmp_a[i])\n",
    "            tmp_a[i] = tmp_min if tmp_min>MIN_MARGIN else MIN_MARGIN\n",
    "        \n",
    "        self.a = np.array(tmp_a).reshape((1, X.shape[0]))\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \n",
    "        X_nums = X.shape[0]\n",
    "        \n",
    "        da=[[]] #(1,100)\n",
    "        dz=[[]] #(1,100)\n",
    "        dw=[0.0, 0.0] #(2)\n",
    "        db=0.0 #()\n",
    "        for i in range(X_nums):\n",
    "            da[0].append(-y[i] / self.a[0][i] + (1 - y[i]) / (1 - self.a[0][i]))\n",
    "            dz[0].append(self.a[0][i] * (1 - self.a[0][i]) * da[0][i])\n",
    "            dw[0] += X[i][0] * dz[0][i]\n",
    "            dw[1] += X[i][1] * dz[0][i]\n",
    "            db += 1 * dz[0][i]\n",
    "            \n",
    "        dw[0]/=X_nums\n",
    "        dw[1]/=X_nums\n",
    "        db/=X_nums\n",
    "        return np.array(dw), db\n",
    "\n",
    "    def train(self, X, y, learning_rate):\n",
    "        self.forward(X)\n",
    "        dw, db = self.backward(X, y)\n",
    "        self.w[0][0] -= learning_rate * dw[0]\n",
    "        self.w[0][1] -= learning_rate * dw[1]\n",
    "        self.b -= learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.round(self.forward(X))\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        pred_y = self.forward(X)\n",
    "        return -np.mean(y * np.log(pred_y) + (1 - y) * np.log(1 - pred_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(sample_num):\n",
    "    X = np.random.randint(-10, 10, (sample_num, 2))\n",
    "    y = (np.sum(X, 1) >0).astype(int)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_samples, test_samples, learning_rate, epochs):\n",
    "    \n",
    "    # model = Model([BinaryClassifier((1,2))])\n",
    "    classifier = BinaryClassifier((1,2))\n",
    "    train_X, train_y = create_samples(train_samples)\n",
    "    test_X, test_y = create_samples(test_samples)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        classifier.train(train_X, train_y, learning_rate)\n",
    "    \n",
    "    return {'w': classifier.w.reshape((1,2)), \n",
    "            'b': classifier.b,\n",
    "            'train_loss': classifier.loss(train_X, train_y),\n",
    "            'test_loss': classifier.loss(test_X, test_y),\n",
    "            'train_acc': 100 * np.mean(classifier.predict(train_X) == train_y),\n",
    "            'test_acc': 100 * np.mean(classifier.predict(test_X) == test_y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_elementwise(train_samples, test_samples, learning_rate, epochs):\n",
    "    \n",
    "    # model = Model([BinaryClassifier((1,2))])\n",
    "    classifier = BinaryClassifierElementwise((1,2))\n",
    "    train_X, train_y = create_samples(train_samples)\n",
    "    test_X, test_y = create_samples(test_samples)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        classifier.train(train_X, train_y, learning_rate)\n",
    "    \n",
    "    return {'w': classifier.w.reshape((1,2)), \n",
    "            'b': classifier.b,\n",
    "            'train_loss': classifier.loss(train_X, train_y),\n",
    "            'test_loss': classifier.loss(test_X, test_y),\n",
    "            'train_acc': 100 * np.mean(classifier.predict(train_X) == train_y),\n",
    "            'test_acc': 100 * np.mean(classifier.predict(test_X) == test_y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': array([[0.37411396, 0.45177562]]),\n",
       " 'b': -0.018992888908487472,\n",
       " 'train_loss': 0.17186290637380514,\n",
       " 'test_loss': 0.1494035394228638,\n",
       " 'train_acc': 96.0,\n",
       " 'test_acc': 97.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = 100 # m\n",
    "test_samples = 100 # n\n",
    "learning_rate = 1e-2\n",
    "epochs = 100 # K\n",
    "\n",
    "train(train_samples, test_samples, learning_rate, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### element-wise version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5377762317657471\n",
      "{'w': array([[0.38266354, 0.43251565]]), 'b': -0.02747253406561454, 'train_loss': 0.17816258634077775, 'test_loss': 0.16564797215229432, 'train_acc': 99.0, 'test_acc': 100.0}\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "result_elementwise = train_elementwise(train_samples, test_samples, learning_rate, epochs)\n",
    "print(time.time()-start)\n",
    "print(result_elementwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012678146362304688\n",
      "{'w': array([[0.41454156, 0.40819955]]), 'b': -0.04905599544328503, 'train_loss': 0.1466250264640928, 'test_loss': 0.15289553856197846, 'train_acc': 98.0, 'test_acc': 100.0}\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "result_vector = train(train_samples, test_samples, learning_rate, epochs)\n",
    "print(time.time()-start)\n",
    "print(result_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameter(train_samples, test_samples, learning_rate,  epochs, runs):\n",
    "    results=[]\n",
    "    for run in range(runs):\n",
    "        results.append(train(train_samples, test_samples, learning_rate,  epochs))\n",
    "    \n",
    "    results_df = pd.DataFrame(results)[['train_loss', 'test_loss', 'train_acc', 'test_acc']]\n",
    "    normalized_result = results_df.sum()/runs\n",
    "    \n",
    "    return normalized_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_loss     0.165415\n",
       "test_loss      0.165954\n",
       "train_acc     98.200000\n",
       "test_acc      98.500000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs = 10\n",
    "\n",
    "test_parameter(train_samples, test_samples, learning_rate, epochs, runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameter m (=train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train_samples :  10\n",
      "train_loss     0.123799\n",
      "test_loss      0.284132\n",
      "train_acc     99.000000\n",
      "test_acc      88.200000\n",
      "dtype: float64\n",
      "\n",
      "Train_samples :  100\n",
      "train_loss     0.176211\n",
      "test_loss      0.175695\n",
      "train_acc     98.400000\n",
      "test_acc      97.700000\n",
      "dtype: float64\n",
      "\n",
      "Train_samples :  1000\n",
      "train_loss     0.171175\n",
      "test_loss      0.162078\n",
      "train_acc     98.630000\n",
      "test_acc      99.200000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for m in [10,100,1000]:\n",
    "    print(\"\\nTrain_samples : \", m)\n",
    "    print(test_parameter(m, test_samples, learning_rate, epochs, runs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameter K (=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs :  10\n",
      "train_loss     0.382770\n",
      "test_loss      0.389212\n",
      "train_acc     97.600000\n",
      "test_acc      96.400000\n",
      "dtype: float64\n",
      "\n",
      "Epochs :  100\n",
      "train_loss     0.175958\n",
      "test_loss      0.165278\n",
      "train_acc     97.600000\n",
      "test_acc      97.100000\n",
      "dtype: float64\n",
      "\n",
      "Epochs :  1000\n",
      "train_loss     0.078167\n",
      "test_loss      0.076554\n",
      "train_acc     99.600000\n",
      "test_acc      98.900000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for K in [10,100,1000]:\n",
    "    print(\"\\nEpochs : \", K)\n",
    "    print(test_parameter(train_samples, test_samples, learning_rate, K, runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameter lr (=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning_rate :  1e-08\n",
      "train_loss     0.693142\n",
      "test_loss      0.693142\n",
      "train_acc     96.400000\n",
      "test_acc      95.600000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  0.0001\n",
      "train_loss     0.637911\n",
      "test_loss      0.641886\n",
      "train_acc     95.500000\n",
      "test_acc      93.000000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  0.01\n",
      "train_loss     0.172011\n",
      "test_loss      0.169323\n",
      "train_acc     98.600000\n",
      "test_acc      98.800000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  0.1\n",
      "train_loss     0.07443\n",
      "test_loss      0.07818\n",
      "train_acc     99.70000\n",
      "test_acc      99.70000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  1.0\n",
      "train_loss      0.02046\n",
      "test_loss       0.03349\n",
      "train_acc     100.00000\n",
      "test_acc       99.80000\n",
      "dtype: float64\n",
      "\n",
      "Learning_rate :  10.0\n",
      "train_loss      0.000645\n",
      "test_loss       0.008518\n",
      "train_acc     100.000000\n",
      "test_acc       99.800000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-8, 1e-4, 1e-2, 1e-1, 1e0, 1e1]:\n",
    "    print(\"\\nLearning_rate : \", lr)\n",
    "    print(test_parameter(train_samples, test_samples, lr, epochs, runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
