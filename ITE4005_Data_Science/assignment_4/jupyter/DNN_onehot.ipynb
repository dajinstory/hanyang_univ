{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Reshape, Concatenate, Dot\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_id = 1):\n",
    "    filename_train = '../data/train/u'+str(dataset_id)+'.base'\n",
    "    filename_test = '../data/train/u'+str(dataset_id)+'.test'\n",
    "    dataset_train_df = pd.read_csv(filename_train, names=['user_id','movie_id','rating','time_stamp'], header=None, delimiter='\\t')\n",
    "    dataset_test_df = pd.read_csv(filename_train, names=['user_id','movie_id','rating','time_stamp'], header=None, delimiter='\\t')\n",
    "    # dataset = dataset_df.to_dict('records')\n",
    "    return dataset_train_df, dataset_test_df  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels):\n",
    "    encodeds = [\n",
    "        [1., .0, .0, .0, .0],\n",
    "        [.0, 1., .0, .0, .0],\n",
    "        [.0, .0, 1., .0, .0],\n",
    "        [.0, .0, .0, 1., .0],\n",
    "        [.0, .0, .0, .0, 1.],\n",
    "    ]\n",
    "    labels_encoded = np.array([encodeds[int(label)-1] for label in labels])\n",
    "    return labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_smoothing(labels):\n",
    "    encodeds = [\n",
    "        [.75, .25, .0, .0, .0],\n",
    "        [.15, .7, .15, .0, .0],\n",
    "        [.0, .15, .7, .15, .0],\n",
    "        [.0, .0, .15, .7, .15],\n",
    "        [.0, .0, .0, .25, .75],\n",
    "    ]\n",
    "    labels_encoded = np.array([encodeds[int(label)-1] for label in labels])\n",
    "    return labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_df, dataset_test_df = load_dataset(dataset_id=1)\n",
    "\n",
    "train_x_user = dataset_train_df['user_id'].values\n",
    "train_x_item = dataset_train_df['movie_id'].values\n",
    "train_y = dataset_train_df['rating'].values\n",
    "train_y_encoded = one_hot_encoding(train_y)\n",
    "train_y_encoded_smoothing = one_hot_encoding_smoothing(train_y)\n",
    "\n",
    "test_x_user = dataset_test_df['user_id'].values\n",
    "test_x_item = dataset_test_df['movie_id'].values\n",
    "test_y = dataset_test_df['rating'].values\n",
    "test_y_encoded = one_hot_encoding(test_y)\n",
    "test_y_encoded_smoothing = one_hot_encoding_smoothing(test_y)\n",
    "\n",
    "num_of_users = dataset_train_df['user_id'].max() + 1\n",
    "num_of_items = dataset_train_df['movie_id'].max() + 1\n",
    "num_of_factors = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our own plot function\n",
    "def scatter(y, pred_y, subtitle='result.png'):\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    \n",
    "    \n",
    "    sc = ax.scatter(y, pred_y, lw=0, s=40)\n",
    "    plt.xlim(-2, 7)\n",
    "    plt.ylim(-2, 7)\n",
    "    if subtitle != None:\n",
    "        plt.suptitle(subtitle)\n",
    "        \n",
    "    plt.savefig(subtitle)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) model #3 - DNN with one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel_onehot(Model):\n",
    "    def __init__(self, num_of_users, num_of_items, num_of_factors):\n",
    "        \n",
    "        # tensor_0 : input\n",
    "        input_user_id = Input((1,), name='input_user_id')\n",
    "        input_item_id = Input((1,), name='input_item_id')\n",
    "        \n",
    "        # model_0 : embedding inpput\n",
    "        model_Embedding_user_id = Sequential([\n",
    "            Embedding(num_of_users, num_of_factors, input_length=1, input_shape=(1,)),\n",
    "            Reshape((num_of_factors,)),\n",
    "        ])\n",
    "        model_Embedding_item_id = Sequential([\n",
    "            Embedding(num_of_items, num_of_factors, input_length=1, input_shape=(1,)),\n",
    "            Reshape((num_of_factors,)),\n",
    "        ])\n",
    "        \n",
    "        # tensor_1 : embedded input\n",
    "        embedded_user_id = model_Embedding_user_id(input_user_id)\n",
    "        embedded_item_id = model_Embedding_item_id(input_item_id)\n",
    "        \n",
    "        # tensor_2 : embedded vector\n",
    "        embedded_vector = Concatenate(axis=-1)([embedded_user_id, embedded_item_id])\n",
    "        \n",
    "        # model_2 : DNN\n",
    "        p_dropout=0.1\n",
    "        model_DNN = Sequential([\n",
    "            Dropout(p_dropout),\n",
    "            Dense(num_of_factors * 2, activation='relu'),\n",
    "            Dropout(p_dropout),\n",
    "            Dense(30, activation='relu'),\n",
    "            Dropout(p_dropout),\n",
    "            Dense(5, activation='softmax'),\n",
    "        ])    \n",
    "        \n",
    "        # tensor_3 : output_label\n",
    "        output_label = model_DNN(embedded_vector)\n",
    "        \n",
    "        super(DNNModel_onehot, self).__init__(\n",
    "            inputs=[input_user_id, input_item_id], \n",
    "            outputs=output_label\n",
    "        )\n",
    "\n",
    "    def score(self, y, pred_y):\n",
    "        return np.sqrt( np.sum(np.square(pred_y.reshape(-1, 1) - y.reshape(-1,1))) / len(y) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - train model and get results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3_a) one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNModel_onehot(num_of_users, num_of_items, num_of_factors)\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    ")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 80000 samples\n",
      "Epoch 1/1000\n",
      "80000/80000 [==============================] - 2s 31us/sample - loss: 0.1426 - val_loss: 0.1308\n",
      "Epoch 2/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1324 - val_loss: 0.1270\n",
      "Epoch 3/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1296 - val_loss: 0.1251\n",
      "Epoch 4/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1277 - val_loss: 0.1227\n",
      "Epoch 5/1000\n",
      "80000/80000 [==============================] - 2s 22us/sample - loss: 0.1261 - val_loss: 0.1212\n",
      "Epoch 6/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1246 - val_loss: 0.1196\n",
      "Epoch 7/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.1234 - val_loss: 0.1179\n",
      "Epoch 8/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1219 - val_loss: 0.1157\n",
      "Epoch 9/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1202 - val_loss: 0.1135\n",
      "Epoch 10/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1183 - val_loss: 0.1107\n",
      "Epoch 11/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1160 - val_loss: 0.1077\n",
      "Epoch 12/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1137 - val_loss: 0.1041\n",
      "Epoch 13/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1112 - val_loss: 0.1002\n",
      "Epoch 14/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1086 - val_loss: 0.0969\n",
      "Epoch 15/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1061 - val_loss: 0.0933\n",
      "Epoch 16/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1033 - val_loss: 0.0901\n",
      "Epoch 17/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1008 - val_loss: 0.0868\n",
      "Epoch 18/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0988 - val_loss: 0.0837\n",
      "Epoch 19/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0966 - val_loss: 0.0810\n",
      "Epoch 20/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0944 - val_loss: 0.0779\n",
      "Epoch 21/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0926 - val_loss: 0.0756\n",
      "Epoch 22/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0908 - val_loss: 0.0736\n",
      "Epoch 23/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0891 - val_loss: 0.0713\n",
      "Epoch 24/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0874 - val_loss: 0.0693\n",
      "Epoch 25/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0863 - val_loss: 0.0674\n",
      "Epoch 26/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0848 - val_loss: 0.0656\n",
      "Epoch 27/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0835 - val_loss: 0.0635\n",
      "Epoch 28/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0822 - val_loss: 0.0624\n",
      "Epoch 29/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0814 - val_loss: 0.0611\n",
      "Epoch 30/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0801 - val_loss: 0.0596\n",
      "Epoch 31/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0789 - val_loss: 0.0582\n",
      "Epoch 32/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0782 - val_loss: 0.0572\n",
      "Epoch 33/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0770 - val_loss: 0.0560\n",
      "Epoch 34/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0765 - val_loss: 0.0552\n",
      "Epoch 35/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0758 - val_loss: 0.0540\n",
      "Epoch 36/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0745 - val_loss: 0.0530\n",
      "Epoch 37/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0735 - val_loss: 0.0518\n",
      "Epoch 38/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0735 - val_loss: 0.0512\n",
      "Epoch 39/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0725 - val_loss: 0.0503\n",
      "Epoch 40/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0723 - val_loss: 0.0496\n",
      "Epoch 41/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0720 - val_loss: 0.0490\n",
      "Epoch 42/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0705 - val_loss: 0.0482\n",
      "Epoch 43/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0704 - val_loss: 0.0474\n",
      "Epoch 44/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0701 - val_loss: 0.0468\n",
      "Epoch 45/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0693 - val_loss: 0.0463\n",
      "Epoch 46/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0691 - val_loss: 0.0456\n",
      "Epoch 47/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0683 - val_loss: 0.0450\n",
      "Epoch 48/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0679 - val_loss: 0.0444\n",
      "Epoch 49/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0675 - val_loss: 0.0439\n",
      "Epoch 50/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0673 - val_loss: 0.0434\n",
      "Epoch 51/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0664 - val_loss: 0.0429\n",
      "Epoch 52/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0662 - val_loss: 0.0426\n",
      "Epoch 53/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0656 - val_loss: 0.0421\n",
      "Epoch 54/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0658 - val_loss: 0.0415\n",
      "Epoch 55/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0653 - val_loss: 0.0409\n",
      "Epoch 56/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0649 - val_loss: 0.0407\n",
      "Epoch 57/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0640 - val_loss: 0.0403\n",
      "Epoch 58/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0641 - val_loss: 0.0399\n",
      "Epoch 59/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0636 - val_loss: 0.0395\n",
      "Epoch 60/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0634 - val_loss: 0.0391\n",
      "Epoch 61/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0633 - val_loss: 0.0388\n",
      "Epoch 62/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0628 - val_loss: 0.0383\n",
      "Epoch 63/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0627 - val_loss: 0.0383\n",
      "Epoch 64/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0626 - val_loss: 0.0377\n",
      "Epoch 65/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0619 - val_loss: 0.0374\n",
      "Epoch 66/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0619 - val_loss: 0.0371\n",
      "Epoch 67/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0619 - val_loss: 0.0368\n",
      "Epoch 68/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0609 - val_loss: 0.0363\n",
      "Epoch 69/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0611 - val_loss: 0.0362\n",
      "Epoch 70/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0607 - val_loss: 0.0358\n",
      "Epoch 71/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0607 - val_loss: 0.0357\n",
      "Epoch 72/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0600 - val_loss: 0.0351\n",
      "Epoch 73/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0594 - val_loss: 0.0351\n",
      "Epoch 74/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0593 - val_loss: 0.0347\n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0592 - val_loss: 0.0344\n",
      "Epoch 76/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0590 - val_loss: 0.0345\n",
      "Epoch 77/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0592 - val_loss: 0.0341\n",
      "Epoch 78/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0587 - val_loss: 0.0336\n",
      "Epoch 79/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0586 - val_loss: 0.0335\n",
      "Epoch 80/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0584 - val_loss: 0.0331\n",
      "Epoch 81/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0582 - val_loss: 0.0329\n",
      "Epoch 82/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0578 - val_loss: 0.0328\n",
      "Epoch 83/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0574 - val_loss: 0.0326\n",
      "Epoch 84/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0576 - val_loss: 0.0323\n",
      "Epoch 85/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0579 - val_loss: 0.0321\n",
      "Epoch 86/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0570 - val_loss: 0.0321\n",
      "Epoch 87/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0571 - val_loss: 0.0316\n",
      "Epoch 88/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0566 - val_loss: 0.0313\n",
      "Epoch 89/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0565 - val_loss: 0.0311\n",
      "Epoch 90/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0564 - val_loss: 0.0309\n",
      "Epoch 91/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0563 - val_loss: 0.0310\n",
      "Epoch 92/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0561 - val_loss: 0.0305\n",
      "Epoch 93/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0563 - val_loss: 0.0306\n",
      "Epoch 94/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0558 - val_loss: 0.0303\n",
      "Epoch 95/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0554 - val_loss: 0.0299\n",
      "Epoch 96/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0556 - val_loss: 0.0299\n",
      "Epoch 97/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0554 - val_loss: 0.0299\n",
      "Epoch 98/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0551 - val_loss: 0.0296\n",
      "Epoch 99/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0548 - val_loss: 0.0293\n",
      "Epoch 100/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0550 - val_loss: 0.0294\n",
      "Epoch 101/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0546 - val_loss: 0.0292\n",
      "Epoch 102/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0549 - val_loss: 0.0290\n",
      "Epoch 103/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0541 - val_loss: 0.0289\n",
      "Epoch 104/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0543 - val_loss: 0.0286\n",
      "Epoch 105/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0540 - val_loss: 0.0287\n",
      "Epoch 106/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0538 - val_loss: 0.0284\n",
      "Epoch 107/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0539 - val_loss: 0.0284\n",
      "Epoch 108/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0539 - val_loss: 0.0280\n",
      "Epoch 109/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0534 - val_loss: 0.0279\n",
      "Epoch 110/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0533 - val_loss: 0.0276\n",
      "Epoch 111/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0537 - val_loss: 0.0280\n",
      "Epoch 112/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0536 - val_loss: 0.0278\n",
      "Epoch 113/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0527 - val_loss: 0.0273\n",
      "Epoch 114/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0535 - val_loss: 0.0274\n",
      "Epoch 115/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0529 - val_loss: 0.0270\n",
      "Epoch 116/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0531 - val_loss: 0.0272\n",
      "Epoch 117/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.0529 - val_loss: 0.0268\n",
      "Epoch 118/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0527 - val_loss: 0.0272\n",
      "Epoch 119/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0521 - val_loss: 0.0265\n",
      "Epoch 120/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0523 - val_loss: 0.0267\n",
      "Epoch 121/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0521 - val_loss: 0.0265\n",
      "Epoch 122/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0523 - val_loss: 0.0262\n",
      "Epoch 123/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0521 - val_loss: 0.0260\n",
      "Epoch 124/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0520 - val_loss: 0.0260\n",
      "Epoch 125/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0520 - val_loss: 0.0260\n",
      "Epoch 126/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0516 - val_loss: 0.0258\n",
      "Epoch 127/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0516 - val_loss: 0.0257\n",
      "Epoch 128/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0513 - val_loss: 0.0255\n",
      "Epoch 129/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0513 - val_loss: 0.0252\n",
      "Epoch 130/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0512 - val_loss: 0.0253\n",
      "Epoch 131/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0513 - val_loss: 0.0251\n",
      "Epoch 132/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0505 - val_loss: 0.0249\n",
      "Epoch 133/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0509 - val_loss: 0.0252\n",
      "Epoch 134/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0507 - val_loss: 0.0249\n",
      "Epoch 135/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0507 - val_loss: 0.0246\n",
      "Epoch 136/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0510 - val_loss: 0.0249\n",
      "Epoch 137/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0508 - val_loss: 0.0247\n",
      "Epoch 138/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0503 - val_loss: 0.0245\n",
      "Epoch 139/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0505 - val_loss: 0.0244\n",
      "Epoch 140/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0505 - val_loss: 0.0244\n",
      "Epoch 141/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0503 - val_loss: 0.0241\n",
      "Epoch 142/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0503 - val_loss: 0.0242\n",
      "Epoch 143/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0496 - val_loss: 0.0240\n",
      "Epoch 144/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0501 - val_loss: 0.0242\n",
      "Epoch 145/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0497 - val_loss: 0.0240\n",
      "Epoch 146/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0497 - val_loss: 0.0238\n",
      "Epoch 147/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0497 - val_loss: 0.0238\n",
      "Epoch 148/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0496 - val_loss: 0.0235\n",
      "Epoch 149/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0495 - val_loss: 0.0237\n",
      "Epoch 150/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0494 - val_loss: 0.0236\n",
      "Epoch 151/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0492 - val_loss: 0.0234\n",
      "Epoch 152/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0491 - val_loss: 0.0234\n",
      "Epoch 153/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0495 - val_loss: 0.0234\n",
      "Epoch 154/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0491 - val_loss: 0.0232\n",
      "Epoch 155/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0494 - val_loss: 0.0232\n",
      "Epoch 156/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0488 - val_loss: 0.0230\n",
      "Epoch 157/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0489 - val_loss: 0.0231\n",
      "Epoch 158/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0489 - val_loss: 0.0230\n",
      "Epoch 159/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0491 - val_loss: 0.0230\n",
      "Epoch 160/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0482 - val_loss: 0.0227\n",
      "Epoch 161/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0486 - val_loss: 0.0227\n",
      "Epoch 162/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0488 - val_loss: 0.0228\n",
      "Epoch 163/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0486 - val_loss: 0.0226\n",
      "Epoch 164/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0485 - val_loss: 0.0223\n",
      "Epoch 165/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0478 - val_loss: 0.0220\n",
      "Epoch 166/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0477 - val_loss: 0.0222\n",
      "Epoch 167/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0481 - val_loss: 0.0222\n",
      "Epoch 168/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0482 - val_loss: 0.0221\n",
      "Epoch 169/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0478 - val_loss: 0.0222\n",
      "Epoch 170/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0480 - val_loss: 0.0219\n",
      "Epoch 171/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0477 - val_loss: 0.0218\n",
      "Epoch 172/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0478 - val_loss: 0.0217\n",
      "Epoch 173/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0481 - val_loss: 0.0219\n",
      "Epoch 174/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0477 - val_loss: 0.0216\n",
      "Epoch 175/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0476 - val_loss: 0.0214\n",
      "Epoch 176/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0472 - val_loss: 0.0215\n",
      "Epoch 177/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0476 - val_loss: 0.0216\n",
      "Epoch 178/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0474 - val_loss: 0.0215\n",
      "Epoch 179/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0472 - val_loss: 0.0213\n",
      "Epoch 180/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0472 - val_loss: 0.0212\n",
      "Epoch 181/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0473 - val_loss: 0.0212\n",
      "Epoch 182/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0467 - val_loss: 0.0212\n",
      "Epoch 183/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0470 - val_loss: 0.0210\n",
      "Epoch 184/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0467 - val_loss: 0.0210\n",
      "Epoch 185/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0466 - val_loss: 0.0210\n",
      "Epoch 186/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0468 - val_loss: 0.0209\n",
      "Epoch 187/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0463 - val_loss: 0.0209\n",
      "Epoch 188/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0462 - val_loss: 0.0208\n",
      "Epoch 189/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0467 - val_loss: 0.0208\n",
      "Epoch 190/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0462 - val_loss: 0.0207\n",
      "Epoch 191/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0463 - val_loss: 0.0206\n",
      "Epoch 192/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0464 - val_loss: 0.0206\n",
      "Epoch 193/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0460 - val_loss: 0.0204\n",
      "Epoch 194/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0461 - val_loss: 0.0203\n",
      "Epoch 195/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0463 - val_loss: 0.0203\n",
      "Epoch 196/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0458 - val_loss: 0.0203\n",
      "Epoch 197/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0459 - val_loss: 0.0205\n",
      "Epoch 198/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0458 - val_loss: 0.0202\n",
      "Epoch 199/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0457 - val_loss: 0.0202\n",
      "Epoch 200/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0460 - val_loss: 0.0203\n",
      "Epoch 201/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0458 - val_loss: 0.0201\n",
      "Epoch 202/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0457 - val_loss: 0.0199\n",
      "Epoch 203/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0459 - val_loss: 0.0199\n",
      "Epoch 204/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0453 - val_loss: 0.0198\n",
      "Epoch 205/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0456 - val_loss: 0.0199\n",
      "Epoch 206/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0459 - val_loss: 0.0197\n",
      "Epoch 207/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0453 - val_loss: 0.0200\n",
      "Epoch 208/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0456 - val_loss: 0.0197\n",
      "Epoch 209/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0453 - val_loss: 0.0196\n",
      "Epoch 210/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0451 - val_loss: 0.0195\n",
      "Epoch 211/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0452 - val_loss: 0.0194\n",
      "Epoch 212/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0454 - val_loss: 0.0193\n",
      "Epoch 213/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0451 - val_loss: 0.0193\n",
      "Epoch 214/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0452 - val_loss: 0.0192\n",
      "Epoch 215/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0451 - val_loss: 0.0193\n",
      "Epoch 216/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0449 - val_loss: 0.0193\n",
      "Epoch 217/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0450 - val_loss: 0.0194\n",
      "Epoch 218/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0449 - val_loss: 0.0192\n",
      "Epoch 219/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0448 - val_loss: 0.0192\n",
      "Epoch 220/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0453 - val_loss: 0.0193\n",
      "Epoch 221/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0450 - val_loss: 0.0192\n",
      "Epoch 222/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0443 - val_loss: 0.0189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0442 - val_loss: 0.0190\n",
      "Epoch 224/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0450 - val_loss: 0.0189\n",
      "Epoch 225/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0443 - val_loss: 0.0188\n",
      "Epoch 226/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0446 - val_loss: 0.0188\n",
      "Epoch 227/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0447 - val_loss: 0.0187\n",
      "Epoch 228/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0446 - val_loss: 0.0187\n",
      "Epoch 229/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0440 - val_loss: 0.0186\n",
      "Epoch 230/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0443 - val_loss: 0.0187\n",
      "Epoch 231/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0440 - val_loss: 0.0186\n",
      "Epoch 232/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0445 - val_loss: 0.0186\n",
      "Epoch 233/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0442 - val_loss: 0.0187\n",
      "Epoch 234/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0439 - val_loss: 0.0185\n",
      "Epoch 235/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0441 - val_loss: 0.0183\n",
      "Epoch 236/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0443 - val_loss: 0.0183\n",
      "Epoch 237/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0440 - val_loss: 0.0182\n",
      "Epoch 238/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0439 - val_loss: 0.0184\n",
      "Epoch 239/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0446 - val_loss: 0.0184\n",
      "Epoch 240/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0437 - val_loss: 0.0182\n",
      "Epoch 241/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0437 - val_loss: 0.0182\n",
      "Epoch 242/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0438 - val_loss: 0.0181\n",
      "Epoch 243/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0442 - val_loss: 0.0181\n",
      "Epoch 244/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0437 - val_loss: 0.0181\n",
      "Epoch 245/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0436 - val_loss: 0.0179\n",
      "Epoch 246/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0438 - val_loss: 0.0180\n",
      "Epoch 247/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0433 - val_loss: 0.0178\n",
      "Epoch 248/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0433 - val_loss: 0.0180\n",
      "Epoch 249/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0434 - val_loss: 0.0179\n",
      "Epoch 250/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0434 - val_loss: 0.0178\n",
      "Epoch 251/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0430 - val_loss: 0.0178\n",
      "Epoch 252/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0434 - val_loss: 0.0178\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_loss', patience=5),\n",
    "]\n",
    "result = model.fit(\n",
    "    [train_x_user, train_x_item], train_y_encoded,\n",
    "    batch_size = 512,\n",
    "    epochs = 1000,\n",
    "    shuffle = True,\n",
    "    verbose = True,\n",
    "    validation_data = ([test_x_user, test_x_item], test_y_encoded),\n",
    "    callbacks = callbacks,\n",
    ")\n",
    "\n",
    "model.save_weights('weights_DNN_onehot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  1.0670344418058866\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_y_encoded = model.predict([test_x_user, test_x_item]).reshape(-1,1)\n",
    "pred_y = pred_y_encoded.reshape(-1,5,).argmax(axis=1)\n",
    "\n",
    "print('RMSE : ', model.score(test_y, pred_y))\n",
    "scatter(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3_b) one-hot encoding with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNModel_onehot(num_of_users, num_of_items, num_of_factors)\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    ")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 80000 samples\n",
      "Epoch 1/1000\n",
      "80000/80000 [==============================] - 2s 25us/sample - loss: 0.0584 - val_loss: 0.0513\n",
      "Epoch 2/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0521 - val_loss: 0.0495\n",
      "Epoch 3/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0508 - val_loss: 0.0486\n",
      "Epoch 4/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0500 - val_loss: 0.0477\n",
      "Epoch 5/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0493 - val_loss: 0.0470\n",
      "Epoch 6/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0486 - val_loss: 0.0463\n",
      "Epoch 7/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0478 - val_loss: 0.0451\n",
      "Epoch 8/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0469 - val_loss: 0.0439\n",
      "Epoch 9/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0460 - val_loss: 0.0426\n",
      "Epoch 10/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0449 - val_loss: 0.0412\n",
      "Epoch 11/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0438 - val_loss: 0.0396\n",
      "Epoch 12/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0426 - val_loss: 0.0382\n",
      "Epoch 13/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0415 - val_loss: 0.0365\n",
      "Epoch 14/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0404 - val_loss: 0.0349\n",
      "Epoch 15/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0392 - val_loss: 0.0333\n",
      "Epoch 16/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0381 - val_loss: 0.0320\n",
      "Epoch 17/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0370 - val_loss: 0.0305\n",
      "Epoch 18/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0362 - val_loss: 0.0294\n",
      "Epoch 19/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0353 - val_loss: 0.0284\n",
      "Epoch 20/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0346 - val_loss: 0.0275\n",
      "Epoch 21/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0338 - val_loss: 0.0267\n",
      "Epoch 22/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0333 - val_loss: 0.0259\n",
      "Epoch 23/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0325 - val_loss: 0.0248\n",
      "Epoch 24/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0319 - val_loss: 0.0242\n",
      "Epoch 25/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0315 - val_loss: 0.0237\n",
      "Epoch 26/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0310 - val_loss: 0.0230\n",
      "Epoch 27/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0304 - val_loss: 0.0223\n",
      "Epoch 28/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0300 - val_loss: 0.0218\n",
      "Epoch 29/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0296 - val_loss: 0.0214\n",
      "Epoch 30/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0292 - val_loss: 0.0208\n",
      "Epoch 31/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0289 - val_loss: 0.0204\n",
      "Epoch 32/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0286 - val_loss: 0.0198\n",
      "Epoch 33/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0282 - val_loss: 0.0197\n",
      "Epoch 34/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0279 - val_loss: 0.0192\n",
      "Epoch 35/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0276 - val_loss: 0.0189\n",
      "Epoch 36/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0273 - val_loss: 0.0186\n",
      "Epoch 37/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0271 - val_loss: 0.0183\n",
      "Epoch 38/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0269 - val_loss: 0.0182\n",
      "Epoch 39/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0266 - val_loss: 0.0176\n",
      "Epoch 40/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0265 - val_loss: 0.0174\n",
      "Epoch 41/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0263 - val_loss: 0.0172\n",
      "Epoch 42/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0258 - val_loss: 0.0169\n",
      "Epoch 43/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0259 - val_loss: 0.0170\n",
      "Epoch 44/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0255 - val_loss: 0.0167\n",
      "Epoch 45/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0254 - val_loss: 0.0162\n",
      "Epoch 46/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0254 - val_loss: 0.0160\n",
      "Epoch 47/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0251 - val_loss: 0.0158\n",
      "Epoch 48/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0249 - val_loss: 0.0156\n",
      "Epoch 49/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0249 - val_loss: 0.0153\n",
      "Epoch 50/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0247 - val_loss: 0.0153\n",
      "Epoch 51/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0242 - val_loss: 0.0151\n",
      "Epoch 52/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0243 - val_loss: 0.0149\n",
      "Epoch 53/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0241 - val_loss: 0.0147\n",
      "Epoch 54/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0240 - val_loss: 0.0147\n",
      "Epoch 55/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0240 - val_loss: 0.0143\n",
      "Epoch 56/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0238 - val_loss: 0.0143\n",
      "Epoch 57/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0237 - val_loss: 0.0142\n",
      "Epoch 58/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0236 - val_loss: 0.0139\n",
      "Epoch 59/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0234 - val_loss: 0.0137\n",
      "Epoch 60/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0233 - val_loss: 0.0138\n",
      "Epoch 61/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0232 - val_loss: 0.0135\n",
      "Epoch 62/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0230 - val_loss: 0.0134\n",
      "Epoch 63/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0229 - val_loss: 0.0136\n",
      "Epoch 64/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0229 - val_loss: 0.0131\n",
      "Epoch 65/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0228 - val_loss: 0.0131\n",
      "Epoch 66/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0226 - val_loss: 0.0130\n",
      "Epoch 67/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0226 - val_loss: 0.0129\n",
      "Epoch 68/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0224 - val_loss: 0.0130\n",
      "Epoch 69/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0224 - val_loss: 0.0126\n",
      "Epoch 70/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0222 - val_loss: 0.0125\n",
      "Epoch 71/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0222 - val_loss: 0.0124\n",
      "Epoch 72/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0222 - val_loss: 0.0123\n",
      "Epoch 73/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0220 - val_loss: 0.0123\n",
      "Epoch 74/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0219 - val_loss: 0.0121\n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0216 - val_loss: 0.0120\n",
      "Epoch 76/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0218 - val_loss: 0.0120\n",
      "Epoch 77/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0217 - val_loss: 0.0117\n",
      "Epoch 78/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0216 - val_loss: 0.0118\n",
      "Epoch 79/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0216 - val_loss: 0.0118\n",
      "Epoch 80/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0216 - val_loss: 0.0117\n",
      "Epoch 81/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0215 - val_loss: 0.0114\n",
      "Epoch 82/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0213 - val_loss: 0.0114\n",
      "Epoch 83/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0212 - val_loss: 0.0113\n",
      "Epoch 84/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0213 - val_loss: 0.0112\n",
      "Epoch 85/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0212 - val_loss: 0.0111\n",
      "Epoch 86/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0211 - val_loss: 0.0112\n",
      "Epoch 87/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0211 - val_loss: 0.0111\n",
      "Epoch 88/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0209 - val_loss: 0.0109\n",
      "Epoch 89/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0208 - val_loss: 0.0109\n",
      "Epoch 90/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0207 - val_loss: 0.0110\n",
      "Epoch 91/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0209 - val_loss: 0.0107\n",
      "Epoch 92/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0207 - val_loss: 0.0109\n",
      "Epoch 93/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0206 - val_loss: 0.0106\n",
      "Epoch 94/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0205 - val_loss: 0.0106\n",
      "Epoch 95/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0203 - val_loss: 0.0106\n",
      "Epoch 96/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0206 - val_loss: 0.0104\n",
      "Epoch 97/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0203 - val_loss: 0.0103\n",
      "Epoch 98/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0204 - val_loss: 0.0104\n",
      "Epoch 99/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0203 - val_loss: 0.0102\n",
      "Epoch 100/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0203 - val_loss: 0.0105\n",
      "Epoch 101/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0202 - val_loss: 0.0101\n",
      "Epoch 102/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0202 - val_loss: 0.0102\n",
      "Epoch 103/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0200 - val_loss: 0.0100\n",
      "Epoch 104/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0200 - val_loss: 0.0100\n",
      "Epoch 105/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0200 - val_loss: 0.0100\n",
      "Epoch 106/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0200 - val_loss: 0.0098\n",
      "Epoch 107/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0200 - val_loss: 0.0100\n",
      "Epoch 108/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0198 - val_loss: 0.0100\n",
      "Epoch 109/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0197 - val_loss: 0.0097\n",
      "Epoch 110/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0197 - val_loss: 0.0097\n",
      "Epoch 111/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0196 - val_loss: 0.0094\n",
      "Epoch 112/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0198 - val_loss: 0.0097\n",
      "Epoch 113/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0196 - val_loss: 0.0094\n",
      "Epoch 114/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0194 - val_loss: 0.0094\n",
      "Epoch 115/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0195 - val_loss: 0.0094\n",
      "Epoch 116/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0194 - val_loss: 0.0094\n",
      "Epoch 117/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0194 - val_loss: 0.0093\n",
      "Epoch 118/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0194 - val_loss: 0.0092\n",
      "Epoch 119/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0194 - val_loss: 0.0092\n",
      "Epoch 120/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0193 - val_loss: 0.0091\n",
      "Epoch 121/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0192 - val_loss: 0.0091\n",
      "Epoch 122/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0192 - val_loss: 0.0092\n",
      "Epoch 123/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0191 - val_loss: 0.0091\n",
      "Epoch 124/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0191 - val_loss: 0.0090\n",
      "Epoch 125/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0191 - val_loss: 0.0090\n",
      "Epoch 126/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0191 - val_loss: 0.0089\n",
      "Epoch 127/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0189 - val_loss: 0.0089\n",
      "Epoch 128/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0190 - val_loss: 0.0088\n",
      "Epoch 129/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0190 - val_loss: 0.0089\n",
      "Epoch 130/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0190 - val_loss: 0.0088\n",
      "Epoch 131/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0189 - val_loss: 0.0089\n",
      "Epoch 132/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0187 - val_loss: 0.0088\n",
      "Epoch 133/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0187 - val_loss: 0.0087\n",
      "Epoch 134/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0188 - val_loss: 0.0086\n",
      "Epoch 135/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0187 - val_loss: 0.0085\n",
      "Epoch 136/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0188 - val_loss: 0.0085\n",
      "Epoch 137/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0186 - val_loss: 0.0085\n",
      "Epoch 138/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0187 - val_loss: 0.0085\n",
      "Epoch 139/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0185 - val_loss: 0.0083\n",
      "Epoch 140/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0185 - val_loss: 0.0085\n",
      "Epoch 141/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0184 - val_loss: 0.0083\n",
      "Epoch 142/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0186 - val_loss: 0.0083\n",
      "Epoch 143/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0186 - val_loss: 0.0083\n",
      "Epoch 144/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0184 - val_loss: 0.0084\n",
      "Epoch 145/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0184 - val_loss: 0.0083\n",
      "Epoch 146/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0184 - val_loss: 0.0083\n",
      "Epoch 147/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0185 - val_loss: 0.0081\n",
      "Epoch 148/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0184 - val_loss: 0.0081\n",
      "Epoch 149/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0184 - val_loss: 0.0083\n",
      "Epoch 150/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0182 - val_loss: 0.0082\n",
      "Epoch 151/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0183 - val_loss: 0.0080\n",
      "Epoch 152/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0182 - val_loss: 0.0081\n",
      "Epoch 153/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0181 - val_loss: 0.0081\n",
      "Epoch 154/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0182 - val_loss: 0.0080\n",
      "Epoch 155/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0182 - val_loss: 0.0080\n",
      "Epoch 156/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0181 - val_loss: 0.0079\n",
      "Epoch 157/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0181 - val_loss: 0.0078\n",
      "Epoch 158/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0182 - val_loss: 0.0080\n",
      "Epoch 159/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0181 - val_loss: 0.0079\n",
      "Epoch 160/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0179 - val_loss: 0.0078\n",
      "Epoch 161/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0180 - val_loss: 0.0077\n",
      "Epoch 162/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0180 - val_loss: 0.0077\n",
      "Epoch 163/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0178 - val_loss: 0.0077\n",
      "Epoch 164/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0179 - val_loss: 0.0077\n",
      "Epoch 165/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0178 - val_loss: 0.0076\n",
      "Epoch 166/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0179 - val_loss: 0.0077\n",
      "Epoch 167/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0178 - val_loss: 0.0076\n",
      "Epoch 168/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0177 - val_loss: 0.0076\n",
      "Epoch 169/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0178 - val_loss: 0.0077\n",
      "Epoch 170/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0178 - val_loss: 0.0075\n",
      "Epoch 171/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0177 - val_loss: 0.0075\n",
      "Epoch 172/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0177 - val_loss: 0.0075\n",
      "Epoch 173/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0177 - val_loss: 0.0075\n",
      "Epoch 174/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0176 - val_loss: 0.0076\n",
      "Epoch 175/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0175 - val_loss: 0.0074\n",
      "Epoch 176/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0176 - val_loss: 0.0074\n",
      "Epoch 177/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0175 - val_loss: 0.0074\n",
      "Epoch 178/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0176 - val_loss: 0.0074\n",
      "Epoch 179/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0176 - val_loss: 0.0075\n",
      "Epoch 180/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0175 - val_loss: 0.0072\n",
      "Epoch 181/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0175 - val_loss: 0.0074\n",
      "Epoch 182/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0174 - val_loss: 0.0073\n",
      "Epoch 183/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0174 - val_loss: 0.0071\n",
      "Epoch 184/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0175 - val_loss: 0.0072\n",
      "Epoch 185/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0175 - val_loss: 0.0073\n",
      "Epoch 186/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0173 - val_loss: 0.0073\n",
      "Epoch 187/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0173 - val_loss: 0.0071\n",
      "Epoch 188/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0173 - val_loss: 0.0073\n",
      "Epoch 189/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0172 - val_loss: 0.0072\n",
      "Epoch 190/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0173 - val_loss: 0.0071\n",
      "Epoch 191/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0172 - val_loss: 0.0072\n",
      "Epoch 192/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0173 - val_loss: 0.0070\n",
      "Epoch 193/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0173 - val_loss: 0.0071\n",
      "Epoch 194/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0171 - val_loss: 0.0070\n",
      "Epoch 195/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0171 - val_loss: 0.0071\n",
      "Epoch 196/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0172 - val_loss: 0.0069\n",
      "Epoch 197/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0173 - val_loss: 0.0070\n",
      "Epoch 198/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0172 - val_loss: 0.0071\n",
      "Epoch 199/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0170 - val_loss: 0.0069\n",
      "Epoch 200/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0171 - val_loss: 0.0068\n",
      "Epoch 201/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0171 - val_loss: 0.0068\n",
      "Epoch 202/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0170 - val_loss: 0.0069\n",
      "Epoch 203/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0169 - val_loss: 0.0069\n",
      "Epoch 204/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0169 - val_loss: 0.0068\n",
      "Epoch 205/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0170 - val_loss: 0.0068\n",
      "Epoch 206/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0170 - val_loss: 0.0068\n",
      "Epoch 207/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0171 - val_loss: 0.0068\n",
      "Epoch 208/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0169 - val_loss: 0.0067\n",
      "Epoch 209/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0169 - val_loss: 0.0068\n",
      "Epoch 210/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0168 - val_loss: 0.0069\n",
      "Epoch 211/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0168 - val_loss: 0.0068\n",
      "Epoch 212/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0169 - val_loss: 0.0067\n",
      "Epoch 213/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0169 - val_loss: 0.0069\n",
      "Epoch 214/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0168 - val_loss: 0.0068\n",
      "Epoch 215/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0167 - val_loss: 0.0067\n",
      "Epoch 216/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0168 - val_loss: 0.0066\n",
      "Epoch 217/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0170 - val_loss: 0.0067\n",
      "Epoch 218/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0169 - val_loss: 0.0065\n",
      "Epoch 219/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0168 - val_loss: 0.0066\n",
      "Epoch 220/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0167 - val_loss: 0.0067\n",
      "Epoch 221/1000\n",
      "80000/80000 [==============================] - 1s 14us/sample - loss: 0.0167 - val_loss: 0.0066\n",
      "Epoch 222/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0167 - val_loss: 0.0066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.0167 - val_loss: 0.0065\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_loss', patience=5),\n",
    "]\n",
    "result = model.fit(\n",
    "    [train_x_user, train_x_item], train_y_encoded_smoothing,\n",
    "    batch_size = 512,\n",
    "    epochs = 1000,\n",
    "    shuffle = True,\n",
    "    verbose = True,\n",
    "    validation_data = ([test_x_user, test_x_item], test_y_encoded_smoothing),\n",
    "    callbacks = callbacks,\n",
    ")\n",
    "\n",
    "model.save_weights('weights_DNN_onehot_smoothing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  1.017693470549949\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_y_encoded = model.predict([test_x_user, test_x_item]).reshape(-1,1)\n",
    "pred_y = pred_y_encoded.reshape(-1,5,).argmax(axis=1)\n",
    "\n",
    "print('RMSE : ', model.score(test_y, pred_y))\n",
    "scatter(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3_c) one-hot encoding - ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNModel_onehot(num_of_users, num_of_items, num_of_factors)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    ")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 80000 samples\n",
      "Epoch 1/1000\n",
      "80000/80000 [==============================] - 2s 26us/sample - loss: 0.4429 - val_loss: 0.4046\n",
      "Epoch 2/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4095 - val_loss: 0.3929\n",
      "Epoch 3/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4006 - val_loss: 0.3866\n",
      "Epoch 4/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3948 - val_loss: 0.3799\n",
      "Epoch 5/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3897 - val_loss: 0.3753\n",
      "Epoch 6/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3853 - val_loss: 0.3693\n",
      "Epoch 7/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3808 - val_loss: 0.3646\n",
      "Epoch 8/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3759 - val_loss: 0.3578\n",
      "Epoch 9/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3707 - val_loss: 0.3512\n",
      "Epoch 10/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3654 - val_loss: 0.3438\n",
      "Epoch 11/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3595 - val_loss: 0.3352\n",
      "Epoch 12/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3536 - val_loss: 0.3275\n",
      "Epoch 13/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3475 - val_loss: 0.3180\n",
      "Epoch 14/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3409 - val_loss: 0.3090\n",
      "Epoch 15/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3347 - val_loss: 0.2992\n",
      "Epoch 16/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3273 - val_loss: 0.2906\n",
      "Epoch 17/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3207 - val_loss: 0.2815\n",
      "Epoch 18/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3154 - val_loss: 0.2732\n",
      "Epoch 19/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3076 - val_loss: 0.2631\n",
      "Epoch 20/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3024 - val_loss: 0.2561\n",
      "Epoch 21/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2954 - val_loss: 0.2470\n",
      "Epoch 22/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2903 - val_loss: 0.2402\n",
      "Epoch 23/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2849 - val_loss: 0.2326\n",
      "Epoch 24/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2804 - val_loss: 0.2260\n",
      "Epoch 25/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2748 - val_loss: 0.2190\n",
      "Epoch 26/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2710 - val_loss: 0.2150\n",
      "Epoch 27/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.2664 - val_loss: 0.2077\n",
      "Epoch 28/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2614 - val_loss: 0.2018\n",
      "Epoch 29/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2578 - val_loss: 0.1968\n",
      "Epoch 30/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2542 - val_loss: 0.1912\n",
      "Epoch 31/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2512 - val_loss: 0.1862\n",
      "Epoch 32/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2473 - val_loss: 0.1827\n",
      "Epoch 33/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2451 - val_loss: 0.1788\n",
      "Epoch 34/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2427 - val_loss: 0.1747\n",
      "Epoch 35/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2394 - val_loss: 0.1704\n",
      "Epoch 36/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2375 - val_loss: 0.1668\n",
      "Epoch 37/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2349 - val_loss: 0.1645\n",
      "Epoch 38/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2320 - val_loss: 0.1609\n",
      "Epoch 39/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2301 - val_loss: 0.1595\n",
      "Epoch 40/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.2271 - val_loss: 0.1556\n",
      "Epoch 41/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2248 - val_loss: 0.1533\n",
      "Epoch 42/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2240 - val_loss: 0.1510\n",
      "Epoch 43/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2209 - val_loss: 0.1478\n",
      "Epoch 44/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2195 - val_loss: 0.1457\n",
      "Epoch 45/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2175 - val_loss: 0.1429\n",
      "Epoch 46/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2163 - val_loss: 0.1414\n",
      "Epoch 47/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2135 - val_loss: 0.1387\n",
      "Epoch 48/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2131 - val_loss: 0.1370\n",
      "Epoch 49/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2108 - val_loss: 0.1345\n",
      "Epoch 50/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2103 - val_loss: 0.1328\n",
      "Epoch 51/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2071 - val_loss: 0.1321\n",
      "Epoch 52/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2068 - val_loss: 0.1311\n",
      "Epoch 53/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2062 - val_loss: 0.1272\n",
      "Epoch 54/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2030 - val_loss: 0.1259\n",
      "Epoch 55/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2029 - val_loss: 0.1243\n",
      "Epoch 56/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2023 - val_loss: 0.1220\n",
      "Epoch 57/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2002 - val_loss: 0.1222\n",
      "Epoch 58/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.2002 - val_loss: 0.1225\n",
      "Epoch 59/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1999 - val_loss: 0.1204\n",
      "Epoch 60/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1982 - val_loss: 0.1188\n",
      "Epoch 61/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1948 - val_loss: 0.1158\n",
      "Epoch 62/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1948 - val_loss: 0.1155\n",
      "Epoch 63/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1945 - val_loss: 0.1155\n",
      "Epoch 64/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1925 - val_loss: 0.1125\n",
      "Epoch 65/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1925 - val_loss: 0.1111\n",
      "Epoch 66/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1920 - val_loss: 0.1105\n",
      "Epoch 67/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.1902 - val_loss: 0.1086\n",
      "Epoch 68/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1908 - val_loss: 0.1111\n",
      "Epoch 69/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1879 - val_loss: 0.1073\n",
      "Epoch 70/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1888 - val_loss: 0.1062\n",
      "Epoch 71/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1877 - val_loss: 0.1053\n",
      "Epoch 72/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1858 - val_loss: 0.1051\n",
      "Epoch 73/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1862 - val_loss: 0.1051\n",
      "Epoch 74/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1850 - val_loss: 0.1042\n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1846 - val_loss: 0.1014\n",
      "Epoch 76/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1828 - val_loss: 0.1005\n",
      "Epoch 77/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1832 - val_loss: 0.1005\n",
      "Epoch 78/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1821 - val_loss: 0.0989\n",
      "Epoch 79/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1827 - val_loss: 0.0985\n",
      "Epoch 80/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.1799 - val_loss: 0.0985\n",
      "Epoch 81/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1797 - val_loss: 0.0971\n",
      "Epoch 82/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1809 - val_loss: 0.0979\n",
      "Epoch 83/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1788 - val_loss: 0.0957\n",
      "Epoch 84/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1778 - val_loss: 0.0938\n",
      "Epoch 85/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1779 - val_loss: 0.0953\n",
      "Epoch 86/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1771 - val_loss: 0.0948\n",
      "Epoch 87/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1762 - val_loss: 0.0915\n",
      "Epoch 88/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1760 - val_loss: 0.0919\n",
      "Epoch 89/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1767 - val_loss: 0.0921\n",
      "Epoch 90/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1748 - val_loss: 0.0908\n",
      "Epoch 91/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1747 - val_loss: 0.0908\n",
      "Epoch 92/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1743 - val_loss: 0.0907\n",
      "Epoch 93/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1734 - val_loss: 0.0907\n",
      "Epoch 94/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1723 - val_loss: 0.0888\n",
      "Epoch 95/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1713 - val_loss: 0.0889\n",
      "Epoch 96/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1719 - val_loss: 0.0878\n",
      "Epoch 97/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1711 - val_loss: 0.0860\n",
      "Epoch 98/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1717 - val_loss: 0.0859\n",
      "Epoch 99/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1711 - val_loss: 0.0869\n",
      "Epoch 100/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1711 - val_loss: 0.0858\n",
      "Epoch 101/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1686 - val_loss: 0.0858\n",
      "Epoch 102/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1703 - val_loss: 0.0848\n",
      "Epoch 103/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1667 - val_loss: 0.0848\n",
      "Epoch 104/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1690 - val_loss: 0.0847\n",
      "Epoch 105/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1682 - val_loss: 0.0850\n",
      "Epoch 106/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1678 - val_loss: 0.0835\n",
      "Epoch 107/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.1664 - val_loss: 0.0819\n",
      "Epoch 108/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1662 - val_loss: 0.0826\n",
      "Epoch 109/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1660 - val_loss: 0.0823\n",
      "Epoch 110/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1650 - val_loss: 0.0811\n",
      "Epoch 111/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1653 - val_loss: 0.0809\n",
      "Epoch 112/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1654 - val_loss: 0.0801\n",
      "Epoch 113/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1648 - val_loss: 0.0801\n",
      "Epoch 114/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1653 - val_loss: 0.0799\n",
      "Epoch 115/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1638 - val_loss: 0.0796\n",
      "Epoch 116/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1636 - val_loss: 0.0799\n",
      "Epoch 117/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1623 - val_loss: 0.0775\n",
      "Epoch 118/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1621 - val_loss: 0.0764\n",
      "Epoch 119/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1624 - val_loss: 0.0768\n",
      "Epoch 120/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.1628 - val_loss: 0.0781\n",
      "Epoch 121/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.1618 - val_loss: 0.0772\n",
      "Epoch 122/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1620 - val_loss: 0.0763\n",
      "Epoch 123/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1608 - val_loss: 0.0753\n",
      "Epoch 124/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1603 - val_loss: 0.0759\n",
      "Epoch 125/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1593 - val_loss: 0.0750\n",
      "Epoch 126/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1586 - val_loss: 0.0746\n",
      "Epoch 127/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1580 - val_loss: 0.0745\n",
      "Epoch 128/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1599 - val_loss: 0.0742\n",
      "Epoch 129/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1589 - val_loss: 0.0735\n",
      "Epoch 130/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1574 - val_loss: 0.0739\n",
      "Epoch 131/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1584 - val_loss: 0.0724\n",
      "Epoch 132/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1584 - val_loss: 0.0731\n",
      "Epoch 133/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1566 - val_loss: 0.0725\n",
      "Epoch 134/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1568 - val_loss: 0.0732\n",
      "Epoch 135/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1554 - val_loss: 0.0714\n",
      "Epoch 136/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1576 - val_loss: 0.0714\n",
      "Epoch 137/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1570 - val_loss: 0.0710\n",
      "Epoch 138/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1556 - val_loss: 0.0709\n",
      "Epoch 139/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1557 - val_loss: 0.0702\n",
      "Epoch 140/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1551 - val_loss: 0.0705\n",
      "Epoch 141/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1555 - val_loss: 0.0694\n",
      "Epoch 142/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1557 - val_loss: 0.0688\n",
      "Epoch 143/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1532 - val_loss: 0.0689\n",
      "Epoch 144/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1539 - val_loss: 0.0698\n",
      "Epoch 145/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1537 - val_loss: 0.0685\n",
      "Epoch 146/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1525 - val_loss: 0.0672\n",
      "Epoch 147/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.1538 - val_loss: 0.0678\n",
      "Epoch 148/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.1527 - val_loss: 0.0675\n",
      "Epoch 149/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.1535 - val_loss: 0.0668\n",
      "Epoch 150/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.1533 - val_loss: 0.0681\n",
      "Epoch 151/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.1539 - val_loss: 0.0683\n",
      "Epoch 152/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.1512 - val_loss: 0.0654\n",
      "Epoch 153/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1524 - val_loss: 0.0671\n",
      "Epoch 154/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1518 - val_loss: 0.0667\n",
      "Epoch 155/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1527 - val_loss: 0.0669\n",
      "Epoch 156/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1506 - val_loss: 0.0651\n",
      "Epoch 157/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1495 - val_loss: 0.0660\n",
      "Epoch 158/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1510 - val_loss: 0.0668\n",
      "Epoch 159/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.1499 - val_loss: 0.0655\n",
      "Epoch 160/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1502 - val_loss: 0.0648\n",
      "Epoch 161/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1503 - val_loss: 0.0648\n",
      "Epoch 162/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1492 - val_loss: 0.0645\n",
      "Epoch 163/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1495 - val_loss: 0.0643\n",
      "Epoch 164/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1482 - val_loss: 0.0637\n",
      "Epoch 165/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1505 - val_loss: 0.0636\n",
      "Epoch 166/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1494 - val_loss: 0.0628\n",
      "Epoch 167/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1488 - val_loss: 0.0630\n",
      "Epoch 168/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1482 - val_loss: 0.0628\n",
      "Epoch 169/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1468 - val_loss: 0.0617\n",
      "Epoch 170/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1466 - val_loss: 0.0613\n",
      "Epoch 171/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1481 - val_loss: 0.0624\n",
      "Epoch 172/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1476 - val_loss: 0.0628\n",
      "Epoch 173/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1476 - val_loss: 0.0618\n",
      "Epoch 174/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1475 - val_loss: 0.0617\n",
      "Epoch 175/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.1467 - val_loss: 0.0616\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_loss', patience=5),\n",
    "]\n",
    "result = model.fit(\n",
    "    [train_x_user, train_x_item], train_y_encoded,\n",
    "    batch_size = 512,\n",
    "    epochs = 1000,\n",
    "    shuffle = True,\n",
    "    verbose = True,\n",
    "    validation_data = ([test_x_user, test_x_item], test_y_encoded),\n",
    "    callbacks = callbacks,\n",
    ")\n",
    "\n",
    "model.save_weights('weights_DNN_onehot_ce.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  1.0322850865918776\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_y_encoded = model.predict([test_x_user, test_x_item]).reshape(-1,1)\n",
    "pred_y = pred_y_encoded.reshape(-1,5,).argmax(axis=1)\n",
    "\n",
    "print('RMSE : ', model.score(test_y, pred_y))\n",
    "scatter(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3_d) one-hot encoding with smoothing - ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNModel_onehot(num_of_users, num_of_items, num_of_factors)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    ")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 80000 samples\n",
      "Epoch 1/1000\n",
      "80000/80000 [==============================] - 2s 27us/sample - loss: 0.4543 - val_loss: 0.4302\n",
      "Epoch 2/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4335 - val_loss: 0.4250\n",
      "Epoch 3/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4288 - val_loss: 0.4212\n",
      "Epoch 4/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4259 - val_loss: 0.4184\n",
      "Epoch 5/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4235 - val_loss: 0.4162\n",
      "Epoch 6/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.4211 - val_loss: 0.4133\n",
      "Epoch 7/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4187 - val_loss: 0.4109\n",
      "Epoch 8/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4162 - val_loss: 0.4078\n",
      "Epoch 9/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4140 - val_loss: 0.4053\n",
      "Epoch 10/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4117 - val_loss: 0.4020\n",
      "Epoch 11/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4093 - val_loss: 0.3992\n",
      "Epoch 12/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4068 - val_loss: 0.3957\n",
      "Epoch 13/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4043 - val_loss: 0.3921\n",
      "Epoch 14/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.4014 - val_loss: 0.3876\n",
      "Epoch 15/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3982 - val_loss: 0.3838\n",
      "Epoch 16/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3954 - val_loss: 0.3799\n",
      "Epoch 17/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3920 - val_loss: 0.3759\n",
      "Epoch 18/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3888 - val_loss: 0.3710\n",
      "Epoch 19/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3857 - val_loss: 0.3679\n",
      "Epoch 20/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3829 - val_loss: 0.3638\n",
      "Epoch 21/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3798 - val_loss: 0.3608\n",
      "Epoch 22/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3776 - val_loss: 0.3576\n",
      "Epoch 23/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3752 - val_loss: 0.3549\n",
      "Epoch 24/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3727 - val_loss: 0.3524\n",
      "Epoch 25/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3714 - val_loss: 0.3499\n",
      "Epoch 26/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3695 - val_loss: 0.3476\n",
      "Epoch 27/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3672 - val_loss: 0.3458\n",
      "Epoch 28/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3662 - val_loss: 0.3438\n",
      "Epoch 29/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3646 - val_loss: 0.3416\n",
      "Epoch 30/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3631 - val_loss: 0.3401\n",
      "Epoch 31/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3622 - val_loss: 0.3390\n",
      "Epoch 32/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3609 - val_loss: 0.3371\n",
      "Epoch 33/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3592 - val_loss: 0.3361\n",
      "Epoch 34/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3584 - val_loss: 0.3350\n",
      "Epoch 35/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3576 - val_loss: 0.3337\n",
      "Epoch 36/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3570 - val_loss: 0.3325\n",
      "Epoch 37/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3558 - val_loss: 0.3310\n",
      "Epoch 38/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3550 - val_loss: 0.3299\n",
      "Epoch 39/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3538 - val_loss: 0.3294\n",
      "Epoch 40/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3536 - val_loss: 0.3279\n",
      "Epoch 41/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3527 - val_loss: 0.3274\n",
      "Epoch 42/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3513 - val_loss: 0.3264\n",
      "Epoch 43/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3510 - val_loss: 0.3258\n",
      "Epoch 44/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3504 - val_loss: 0.3248\n",
      "Epoch 45/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3498 - val_loss: 0.3240\n",
      "Epoch 46/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3492 - val_loss: 0.3232\n",
      "Epoch 47/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3486 - val_loss: 0.3227\n",
      "Epoch 48/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3480 - val_loss: 0.3220\n",
      "Epoch 49/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3474 - val_loss: 0.3215\n",
      "Epoch 50/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3471 - val_loss: 0.3210\n",
      "Epoch 51/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3464 - val_loss: 0.3200\n",
      "Epoch 52/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3462 - val_loss: 0.3197\n",
      "Epoch 53/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3456 - val_loss: 0.3194\n",
      "Epoch 54/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3450 - val_loss: 0.3191\n",
      "Epoch 55/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3449 - val_loss: 0.3181\n",
      "Epoch 56/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3443 - val_loss: 0.3176\n",
      "Epoch 57/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3437 - val_loss: 0.3174\n",
      "Epoch 58/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3440 - val_loss: 0.3166\n",
      "Epoch 59/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3431 - val_loss: 0.3163\n",
      "Epoch 60/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3426 - val_loss: 0.3162\n",
      "Epoch 61/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3421 - val_loss: 0.3154\n",
      "Epoch 62/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3419 - val_loss: 0.3147\n",
      "Epoch 63/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3416 - val_loss: 0.3151\n",
      "Epoch 64/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3413 - val_loss: 0.3149\n",
      "Epoch 65/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3410 - val_loss: 0.3137\n",
      "Epoch 66/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3408 - val_loss: 0.3135\n",
      "Epoch 67/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3401 - val_loss: 0.3131\n",
      "Epoch 68/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3398 - val_loss: 0.3129\n",
      "Epoch 69/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3395 - val_loss: 0.3119\n",
      "Epoch 70/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3396 - val_loss: 0.3121\n",
      "Epoch 71/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3393 - val_loss: 0.3112\n",
      "Epoch 72/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3393 - val_loss: 0.3112\n",
      "Epoch 73/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3388 - val_loss: 0.3111\n",
      "Epoch 74/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3387 - val_loss: 0.3104\n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3381 - val_loss: 0.3107\n",
      "Epoch 76/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3373 - val_loss: 0.3103\n",
      "Epoch 77/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3375 - val_loss: 0.3096\n",
      "Epoch 78/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3371 - val_loss: 0.3097\n",
      "Epoch 79/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3367 - val_loss: 0.3090\n",
      "Epoch 80/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3368 - val_loss: 0.3095\n",
      "Epoch 81/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3366 - val_loss: 0.3090\n",
      "Epoch 82/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3359 - val_loss: 0.3083\n",
      "Epoch 83/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3364 - val_loss: 0.3083\n",
      "Epoch 84/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3359 - val_loss: 0.3085\n",
      "Epoch 85/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3357 - val_loss: 0.3079\n",
      "Epoch 86/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3354 - val_loss: 0.3070\n",
      "Epoch 87/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3354 - val_loss: 0.3070\n",
      "Epoch 88/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3348 - val_loss: 0.3069\n",
      "Epoch 89/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3348 - val_loss: 0.3064\n",
      "Epoch 90/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3344 - val_loss: 0.3067\n",
      "Epoch 91/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3344 - val_loss: 0.3065\n",
      "Epoch 92/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3342 - val_loss: 0.3058\n",
      "Epoch 93/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3340 - val_loss: 0.3058\n",
      "Epoch 94/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3336 - val_loss: 0.3052\n",
      "Epoch 95/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3330 - val_loss: 0.3055\n",
      "Epoch 96/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3333 - val_loss: 0.3054\n",
      "Epoch 97/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3339 - val_loss: 0.3053\n",
      "Epoch 98/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3333 - val_loss: 0.3049\n",
      "Epoch 99/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3329 - val_loss: 0.3052\n",
      "Epoch 100/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3334 - val_loss: 0.3046\n",
      "Epoch 101/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3327 - val_loss: 0.3046\n",
      "Epoch 102/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3329 - val_loss: 0.3045\n",
      "Epoch 103/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3326 - val_loss: 0.3038\n",
      "Epoch 104/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3323 - val_loss: 0.3034\n",
      "Epoch 105/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3324 - val_loss: 0.3036\n",
      "Epoch 106/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3319 - val_loss: 0.3037\n",
      "Epoch 107/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3316 - val_loss: 0.3035\n",
      "Epoch 108/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3320 - val_loss: 0.3031\n",
      "Epoch 109/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3313 - val_loss: 0.3032\n",
      "Epoch 110/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3316 - val_loss: 0.3033\n",
      "Epoch 111/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3315 - val_loss: 0.3032\n",
      "Epoch 112/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3307 - val_loss: 0.3026\n",
      "Epoch 113/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3311 - val_loss: 0.3024\n",
      "Epoch 114/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3310 - val_loss: 0.3029\n",
      "Epoch 115/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3307 - val_loss: 0.3018\n",
      "Epoch 116/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3308 - val_loss: 0.3021\n",
      "Epoch 117/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3306 - val_loss: 0.3016\n",
      "Epoch 118/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3306 - val_loss: 0.3018\n",
      "Epoch 119/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3304 - val_loss: 0.3020\n",
      "Epoch 120/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3300 - val_loss: 0.3018\n",
      "Epoch 121/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3297 - val_loss: 0.3013\n",
      "Epoch 122/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3292 - val_loss: 0.3011\n",
      "Epoch 123/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3297 - val_loss: 0.3011\n",
      "Epoch 124/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3299 - val_loss: 0.3011\n",
      "Epoch 125/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3293 - val_loss: 0.3010\n",
      "Epoch 126/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3299 - val_loss: 0.3010\n",
      "Epoch 127/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3290 - val_loss: 0.3008\n",
      "Epoch 128/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3292 - val_loss: 0.3006\n",
      "Epoch 129/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3293 - val_loss: 0.3011\n",
      "Epoch 130/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3289 - val_loss: 0.3004\n",
      "Epoch 131/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3288 - val_loss: 0.3006\n",
      "Epoch 132/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3283 - val_loss: 0.3002\n",
      "Epoch 133/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3287 - val_loss: 0.3001\n",
      "Epoch 134/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3284 - val_loss: 0.2999\n",
      "Epoch 135/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3285 - val_loss: 0.2997\n",
      "Epoch 136/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3286 - val_loss: 0.2998\n",
      "Epoch 137/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3282 - val_loss: 0.3000\n",
      "Epoch 138/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3281 - val_loss: 0.2994\n",
      "Epoch 139/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3278 - val_loss: 0.2995\n",
      "Epoch 140/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3284 - val_loss: 0.2995\n",
      "Epoch 141/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3275 - val_loss: 0.2994\n",
      "Epoch 142/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3277 - val_loss: 0.2991\n",
      "Epoch 143/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3279 - val_loss: 0.2991\n",
      "Epoch 144/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3274 - val_loss: 0.2987\n",
      "Epoch 145/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3281 - val_loss: 0.2992\n",
      "Epoch 146/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3271 - val_loss: 0.2987\n",
      "Epoch 147/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3270 - val_loss: 0.2988\n",
      "Epoch 148/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3271 - val_loss: 0.2985\n",
      "Epoch 149/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3273 - val_loss: 0.2986\n",
      "Epoch 150/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3268 - val_loss: 0.2987\n",
      "Epoch 151/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3271 - val_loss: 0.2981\n",
      "Epoch 152/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3270 - val_loss: 0.2981\n",
      "Epoch 153/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3266 - val_loss: 0.2983\n",
      "Epoch 154/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3269 - val_loss: 0.2978\n",
      "Epoch 155/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3268 - val_loss: 0.2977\n",
      "Epoch 156/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3263 - val_loss: 0.2976\n",
      "Epoch 157/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3264 - val_loss: 0.2974\n",
      "Epoch 158/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3264 - val_loss: 0.2974\n",
      "Epoch 159/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3261 - val_loss: 0.2974\n",
      "Epoch 160/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3260 - val_loss: 0.2974\n",
      "Epoch 161/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3258 - val_loss: 0.2972\n",
      "Epoch 162/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3260 - val_loss: 0.2974\n",
      "Epoch 163/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3259 - val_loss: 0.2972\n",
      "Epoch 164/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3256 - val_loss: 0.2967\n",
      "Epoch 165/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3258 - val_loss: 0.2968\n",
      "Epoch 166/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3258 - val_loss: 0.2970\n",
      "Epoch 167/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3256 - val_loss: 0.2969\n",
      "Epoch 168/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3252 - val_loss: 0.2966\n",
      "Epoch 169/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3256 - val_loss: 0.2967\n",
      "Epoch 170/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3254 - val_loss: 0.2970\n",
      "Epoch 171/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3250 - val_loss: 0.2963\n",
      "Epoch 172/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3251 - val_loss: 0.2967\n",
      "Epoch 173/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3251 - val_loss: 0.2962\n",
      "Epoch 174/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3249 - val_loss: 0.2963\n",
      "Epoch 175/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3252 - val_loss: 0.2962\n",
      "Epoch 176/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3254 - val_loss: 0.2969\n",
      "Epoch 177/1000\n",
      "80000/80000 [==============================] - 3s 39us/sample - loss: 0.3249 - val_loss: 0.2962\n",
      "Epoch 178/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.3246 - val_loss: 0.2962\n",
      "Epoch 179/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3251 - val_loss: 0.2959\n",
      "Epoch 180/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3253 - val_loss: 0.2965\n",
      "Epoch 181/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3244 - val_loss: 0.2959\n",
      "Epoch 182/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3241 - val_loss: 0.2956\n",
      "Epoch 183/1000\n",
      "80000/80000 [==============================] - 1s 15us/sample - loss: 0.3246 - val_loss: 0.2960\n",
      "Epoch 184/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3241 - val_loss: 0.2957\n",
      "Epoch 185/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3244 - val_loss: 0.2958\n",
      "Epoch 186/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3240 - val_loss: 0.2957\n",
      "Epoch 187/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.3241 - val_loss: 0.2953\n",
      "Epoch 188/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3243 - val_loss: 0.2957\n",
      "Epoch 189/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3240 - val_loss: 0.2957\n",
      "Epoch 190/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3242 - val_loss: 0.2955\n",
      "Epoch 191/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3244 - val_loss: 0.2954\n",
      "Epoch 192/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3238 - val_loss: 0.2952\n",
      "Epoch 193/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3238 - val_loss: 0.2952\n",
      "Epoch 194/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3234 - val_loss: 0.2950\n",
      "Epoch 195/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3238 - val_loss: 0.2948\n",
      "Epoch 196/1000\n",
      "80000/80000 [==============================] - 1s 16us/sample - loss: 0.3236 - val_loss: 0.2945\n",
      "Epoch 197/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3236 - val_loss: 0.2951\n",
      "Epoch 198/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.3234 - val_loss: 0.2950\n",
      "Epoch 199/1000\n",
      "80000/80000 [==============================] - 2s 19us/sample - loss: 0.3238 - val_loss: 0.2945\n",
      "Epoch 200/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.3234 - val_loss: 0.2942\n",
      "Epoch 201/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.3235 - val_loss: 0.2944\n",
      "Epoch 202/1000\n",
      "80000/80000 [==============================] - 1s 19us/sample - loss: 0.3232 - val_loss: 0.2944\n",
      "Epoch 203/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.3232 - val_loss: 0.2945\n",
      "Epoch 204/1000\n",
      "80000/80000 [==============================] - 2s 19us/sample - loss: 0.3224 - val_loss: 0.2940\n",
      "Epoch 205/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.3232 - val_loss: 0.2939\n",
      "Epoch 206/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.3230 - val_loss: 0.2942\n",
      "Epoch 207/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3233 - val_loss: 0.2940\n",
      "Epoch 208/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3228 - val_loss: 0.2941\n",
      "Epoch 209/1000\n",
      "80000/80000 [==============================] - 1s 17us/sample - loss: 0.3229 - val_loss: 0.2943\n",
      "Epoch 210/1000\n",
      "80000/80000 [==============================] - 1s 18us/sample - loss: 0.3225 - val_loss: 0.2941\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_loss', patience=5),\n",
    "]\n",
    "result = model.fit(\n",
    "    [train_x_user, train_x_item], train_y_encoded_smoothing,\n",
    "    batch_size = 512,\n",
    "    epochs = 1000,\n",
    "    shuffle = True,\n",
    "    verbose = True,\n",
    "    validation_data = ([test_x_user, test_x_item], test_y_encoded_smoothing),\n",
    "    callbacks = callbacks,\n",
    ")\n",
    "\n",
    "model.save_weights('weights_DNN_onehot_smoothing_ce.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  1.0284454287904634\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_y_encoded = model.predict([test_x_user, test_x_item]).reshape(-1,1)\n",
    "pred_y = pred_y_encoded.reshape(-1,5,).argmax(axis=1)\n",
    "\n",
    "print('RMSE : ', model.score(test_y, pred_y))\n",
    "scatter(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
